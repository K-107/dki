{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "211104_689Ï¢Ö_Î™©Î°ù.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/K-107/dki/blob/main/Food_Detection/211207_YOLOv5_%EB%8F%99%EC%98%81%EC%83%81.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ÏùåÏãù Î™©Î°ù Í∞ÄÏ†∏ÏôÄÏÑú modelÏùò Ï∂úÎ†• Í∞íÍ≥º ÎßûÏ∂∞Ï§Ñ Ï§ÄÎπÑ"
      ],
      "metadata": {
        "id": "iK0djoqq8Mtp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ON-gSgXCag1y",
        "outputId": "0b2532ee-c774-41d8-ccae-a316506044aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8chNnKmadOq",
        "outputId": "c00d3c6e-1203-4a81-ede4-161ef8ebdfd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install openpyxl"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.7/dist-packages (2.5.9)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: jdcal in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTYmRYH8cd9b"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, glob\n",
        "import shutil\n",
        "\n",
        "df = pd.read_excel('/content/drive/MyDrive/okra/·Ñå·Ö©·ÑÖ·Öµ·Ñã·Ö≥·Ü∑·Ñâ·Öµ·Ü®·ÑÜ·Ö©·Ü®·ÑÖ·Ö©·Ü® ·ÑÜ·Öµ·Üæ ·Ñí·Öß·Ü´·Ñí·Ö™·Üº.xlsx', engine='openpyxl') # ÏóëÏÖÄ ÌååÏùº Î°úÎìú\n",
        "df = df.iloc[:,1:3].rename(columns={'ÎùºÎ≤® Ïù∏Îç±Ïä§':'number','FOOD_NM':'name'})\n",
        "df.dropna(inplace=True)\n",
        "df.number = df.number.astype(int)\n",
        "df = df.drop_duplicates('number')\n",
        "for i in range(1000):\n",
        "  if i not in df.number.to_list():\n",
        "    df = df.append({'number':i,'name':'NONE'}, ignore_index=True)\n",
        "df = df.set_index('number')\n",
        "df_sort = df.sort_values('number')\n",
        "names = df_sort.name.to_list()"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names[641]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "R9AAL7gclw0D",
        "outputId": "a4d99f53-b559-4161-91a4-de9b026f190f"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ÎØ∏Ïó≠Ï§ÑÍ∏∞Ïû°Ï±Ñ'"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YOLOv5 ÍπÉÌÅ¥Î°†"
      ],
      "metadata": {
        "id": "1fHkTSXd8ZwG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUJzMoFQbwml",
        "outputId": "7b56b298-d2d3-4eb8-a18b-776de0be7308",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt  # install\n",
        "\n",
        "from yolov5 import utils\n",
        "display = utils.notebook_init()  # checks"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 üöÄ v6.0-127-g554f782 torch 1.10.0+cu111 CUDA:0 (Tesla K80, 11441MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ‚úÖ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDRzrmCWrpYX",
        "outputId": "edb3e964-f3c3-4e05-e27f-a84ea3ef769a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://www.wfonts.com/download/data/2016/06/13/malgun-gothic/malgun.ttf\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwLw9zpXjRr4",
        "outputId": "24dd95a3-7dea-403d-d3e6-25e197b171ac"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-08 05:08:34--  https://www.wfonts.com/download/data/2016/06/13/malgun-gothic/malgun.ttf\n",
            "Resolving www.wfonts.com (www.wfonts.com)... 104.225.219.210\n",
            "Connecting to www.wfonts.com (www.wfonts.com)|104.225.219.210|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9595100 (9.2M) [application/octetstream]\n",
            "Saving to: ‚Äòmalgun.ttf‚Äô\n",
            "\n",
            "malgun.ttf          100%[===================>]   9.15M  34.4MB/s    in 0.3s    \n",
            "\n",
            "2021-12-08 05:08:34 (34.4 MB/s) - ‚Äòmalgun.ttf‚Äô saved [9595100/9595100]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDWChPJdyhol"
      },
      "source": [
        "# detect.py ÌååÏùº ÍµêÏ≤¥"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-BjBcvuy5Lm"
      },
      "source": [
        "# YOLOv5 üöÄ by Ultralytics, GPL-3.0 license\n",
        "\"\"\"\n",
        "Run inference on images, videos, directories, streams, etc.\n",
        "\n",
        "Usage:\n",
        "    $ python path/to/detect.py --weights yolov5s.pt --source 0  # webcam\n",
        "                                                             img.jpg  # image\n",
        "                                                             vid.mp4  # video\n",
        "                                                             path/  # directory\n",
        "                                                             path/*.jpg  # glob\n",
        "                                                             'https://youtu.be/Zgi9g1ksQHc'  # YouTube\n",
        "                                                             'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, glob\n",
        "import shutil\n",
        "\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "FILE = Path(__file__).resolve()\n",
        "ROOT = FILE.parents[0]  # YOLOv5 root directory\n",
        "if str(ROOT) not in sys.path:\n",
        "    sys.path.append(str(ROOT))  # add ROOT to PATH\n",
        "ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n",
        "\n",
        "from models.common import DetectMultiBackend\n",
        "from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def run(weights=ROOT / 'yolov5s.pt',  # model.pt path(s)\n",
        "        source=ROOT / 'data/images',  # file/dir/URL/glob, 0 for webcam\n",
        "        imgsz=640,  # inference size (pixels)\n",
        "        conf_thres=0.25,  # confidence threshold\n",
        "        iou_thres=0.45,  # NMS IOU threshold\n",
        "        max_det=1000,  # maximum detections per image\n",
        "        device='',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
        "        view_img=False,  # show results\n",
        "        save_txt=False,  # save results to *.txt\n",
        "        save_conf=False,  # save confidences in --save-txt labels\n",
        "        save_crop=False,  # save cropped prediction boxes\n",
        "        nosave=False,  # do not save images/videos\n",
        "        classes=None,  # filter by class: --class 0, or --class 0 2 3\n",
        "        agnostic_nms=False,  # class-agnostic NMS\n",
        "        augment=False,  # augmented inference\n",
        "        visualize=False,  # visualize features\n",
        "        update=False,  # update all models\n",
        "        project=ROOT / 'runs/detect',  # save results to project/name\n",
        "        name='exp',  # save results to project/name\n",
        "        exist_ok=False,  # existing project/name ok, do not increment\n",
        "        line_thickness=3,  # bounding box thickness (pixels)\n",
        "        hide_labels=False,  # hide labels\n",
        "        hide_conf=False,  # hide confidences\n",
        "        half=False,  # use FP16 half-precision inference\n",
        "        dnn=False,  # use OpenCV DNN for ONNX inference\n",
        "        ):\n",
        "    source = str(source)\n",
        "    save_img = not nosave and not source.endswith('.txt')  # save inference images\n",
        "    is_file = Path(source).suffix[1:] in (IMG_FORMATS + VID_FORMATS)\n",
        "    is_url = source.lower().startswith(('rtsp://', 'rtmp://', 'http://', 'https://'))\n",
        "    webcam = source.isnumeric() or source.endswith('.txt') or (is_url and not is_file)\n",
        "    if is_url and is_file:\n",
        "        source = check_file(source)  # download\n",
        "\n",
        "    # Directories\n",
        "    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\n",
        "    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
        "\n",
        "    # Load model\n",
        "################################ model.names ÏÇ≠Ï†ú Î∞è ÍµêÏ≤¥ ###############################################\n",
        "\n",
        "    df = pd.read_excel('/content/drive/MyDrive/okra/·Ñå·Ö©·ÑÖ·Öµ·Ñã·Ö≥·Ü∑·Ñâ·Öµ·Ü®·ÑÜ·Ö©·Ü®·ÑÖ·Ö©·Ü® ·ÑÜ·Öµ·Üæ ·Ñí·Öß·Ü´·Ñí·Ö™·Üº.xlsx', engine='openpyxl') # ÏóëÏÖÄ ÌååÏùº Î°úÎìú\n",
        "    df = df.iloc[:,1:3].rename(columns={'ÎùºÎ≤® Ïù∏Îç±Ïä§':'number','FOOD_NM':'name'})\n",
        "    df.dropna(inplace=True)\n",
        "    df.number = df.number.astype(int)\n",
        "    df = df.drop_duplicates('number')\n",
        "    for i in range(1000):\n",
        "      if i not in df.number.to_list():\n",
        "        df = df.append({'number':i,'name':'NONE'}, ignore_index=True)\n",
        "    df = df.set_index('number')\n",
        "    df_sort = df.sort_values('number')\n",
        "    names = df_sort.name.to_list()\n",
        "\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weights, device=device, dnn=dnn)\n",
        "    stride,  pt, jit, onnx, engine = model.stride, model.pt, model.jit, model.onnx, model.engine\n",
        "##########################################################################################################\n",
        "    imgsz = check_img_size(imgsz, s=stride)  # check image size\n",
        "\n",
        "    # Half\n",
        "    half &= (pt or jit or engine) and device.type != 'cpu'  # half precision only supported by PyTorch on CUDA\n",
        "    if pt or jit:\n",
        "        model.model.half() if half else model.model.float()\n",
        "\n",
        "    # Dataloader\n",
        "    if webcam:\n",
        "        view_img = check_imshow()\n",
        "        cudnn.benchmark = True  # set True to speed up constant image size inference\n",
        "        dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt)\n",
        "        bs = len(dataset)  # batch_size\n",
        "    else:\n",
        "        dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt)\n",
        "        bs = 1  # batch_size\n",
        "    vid_path, vid_writer = [None] * bs, [None] * bs\n",
        "\n",
        "    # Run inference\n",
        "    model.warmup(imgsz=(1, 3, *imgsz), half=half)  # warmup\n",
        "    dt, seen = [0.0, 0.0, 0.0], 0\n",
        "    for path, im, im0s, vid_cap, s in dataset:\n",
        "        t1 = time_sync()\n",
        "        im = torch.from_numpy(im).to(device)\n",
        "        im = im.half() if half else im.float()  # uint8 to fp16/32\n",
        "        im /= 255  # 0 - 255 to 0.0 - 1.0\n",
        "        if len(im.shape) == 3:\n",
        "            im = im[None]  # expand for batch dim\n",
        "        t2 = time_sync()\n",
        "        dt[0] += t2 - t1\n",
        "\n",
        "        # Inference\n",
        "        visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n",
        "        pred = model(im, augment=augment, visualize=visualize)\n",
        "        t3 = time_sync()\n",
        "        dt[1] += t3 - t2\n",
        "\n",
        "        # NMS\n",
        "        pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
        "\n",
        "      \n",
        "\n",
        "        # print('pred: ',int(pred[0][0][5]))\n",
        "        # pred[0][0][5] = torch.tensor(label_[int(pred[0][0][5])])\n",
        "        # print('change: ', label_[int(pred[0][0][5])])\n",
        "\n",
        "        dt[2] += time_sync() - t3\n",
        "\n",
        "        # Second-stage classifier (optional)\n",
        "        # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)\n",
        "\n",
        "        # Process predictions\n",
        "        for i, det in enumerate(pred):  # per image\n",
        "            seen += 1\n",
        "            if webcam:  # batch_size >= 1\n",
        "                p, im0, frame = path[i], im0s[i].copy(), dataset.count\n",
        "                # print('p:', p, ', im0: ',im0, ',frame: ', frame)\n",
        "                s += f'{i}: '\n",
        "            else:\n",
        "                p, im0, frame = path, im0s.copy(), getattr(dataset, 'frame', 0)\n",
        "                # print('p:', p, ', im0: ',im0, ',frame: ', frame)\n",
        "\n",
        "            p = Path(p)  # to Path\n",
        "            save_path = str(save_dir / p.name)  # im.jpg\n",
        "            txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # im.txt\n",
        "            s += '%gx%g ' % im.shape[2:]  # print string\n",
        "            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
        "            imc = im0.copy() if save_crop else im0  # for save_crop\n",
        "            annotator = Annotator(im0, line_width=line_thickness, example=str(names))\n",
        "            if len(det):\n",
        "                # Rescale boxes from img_size to im0 size\n",
        "                det[:, :4] = scale_coords(im.shape[2:], det[:, :4], im0.shape).round()\n",
        "                # print('det: ', det)\n",
        "\n",
        "                # Print results\n",
        "                for c in det[:, -1].unique():\n",
        "                    n = (det[:, -1] == c).sum()  # detections per class\n",
        "                    s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
        "                    # print('s: ', s)\n",
        "\n",
        "                # Write results\n",
        "                for *xyxy, conf, cls in reversed(det):\n",
        "                    if save_txt:  # Write to file\n",
        "                        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
        "                        line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\n",
        "                        with open(txt_path + '.txt', 'a') as f:\n",
        "                            f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
        "\n",
        "                    if save_img or save_crop or view_img:  # Add bbox to image\n",
        "                        c = int(cls)  # integer class\n",
        "                        label = None if hide_labels else (names[c] if hide_conf else f'{names[c]} {conf:.2f}')\n",
        "                        annotator.box_label(xyxy, label, color=colors(c, True))\n",
        "                        if save_crop:\n",
        "                            save_one_box(xyxy, imc, file=save_dir / 'crops' / names[c] / f'{p.stem}.jpg', BGR=True)\n",
        "\n",
        "            # Print time (inference-only)\n",
        "            LOGGER.info(f'{s}Done. ({t3 - t2:.3f}s)')\n",
        "\n",
        "            # Stream results\n",
        "            im0 = annotator.result()\n",
        "            if view_img:\n",
        "                cv2.imshow(str(p), im0)\n",
        "                cv2.waitKey(1)  # 1 millisecond\n",
        "\n",
        "            # Save results (image with detections)\n",
        "            if save_img:\n",
        "                if dataset.mode == 'image':\n",
        "                    cv2.imwrite(save_path, im0)\n",
        "                else:  # 'video' or 'stream'\n",
        "                    if vid_path[i] != save_path:  # new video\n",
        "                        vid_path[i] = save_path\n",
        "                        if isinstance(vid_writer[i], cv2.VideoWriter):\n",
        "                            vid_writer[i].release()  # release previous video writer\n",
        "                        if vid_cap:  # video\n",
        "                            fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
        "                            w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "                            h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "                        else:  # stream\n",
        "                            fps, w, h = 30, im0.shape[1], im0.shape[0]\n",
        "                            save_path += '.mp4'\n",
        "                        vid_writer[i] = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
        "                    vid_writer[i].write(im0)\n",
        "\n",
        "    # Print results\n",
        "    t = tuple(x / seen * 1E3 for x in dt)  # speeds per image\n",
        "    LOGGER.info(f'Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {(1, 3, *imgsz)}' % t)\n",
        "    if save_txt or save_img:\n",
        "        s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else ''\n",
        "        LOGGER.info(f\"Results saved to {colorstr('bold', save_dir)}{s}\")\n",
        "    if update:\n",
        "        strip_optimizer(weights)  # update model (to fix SourceChangeWarning)\n",
        "\n",
        "\n",
        "def parse_opt():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model path(s)')\n",
        "    parser.add_argument('--source', type=str, default=ROOT / 'data/images', help='file/dir/URL/glob, 0 for webcam')\n",
        "    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640], help='inference size h,w')\n",
        "    parser.add_argument('--conf-thres', type=float, default=0.25, help='confidence threshold')\n",
        "    parser.add_argument('--iou-thres', type=float, default=0.45, help='NMS IoU threshold')\n",
        "    parser.add_argument('--max-det', type=int, default=1000, help='maximum detections per image')\n",
        "    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
        "    parser.add_argument('--view-img', action='store_true', help='show results')\n",
        "    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')\n",
        "    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')\n",
        "    parser.add_argument('--save-crop', action='store_true', help='save cropped prediction boxes')\n",
        "    parser.add_argument('--nosave', action='store_true', help='do not save images/videos')\n",
        "    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --classes 0, or --classes 0 2 3')\n",
        "    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')\n",
        "    parser.add_argument('--augment', action='store_true', help='augmented inference')\n",
        "    parser.add_argument('--visualize', action='store_true', help='visualize features')\n",
        "    parser.add_argument('--update', action='store_true', help='update all models')\n",
        "    parser.add_argument('--project', default=ROOT / 'runs/detect', help='save results to project/name')\n",
        "    parser.add_argument('--name', default='exp', help='save results to project/name')\n",
        "    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n",
        "    parser.add_argument('--line-thickness', default=3, type=int, help='bounding box thickness (pixels)')\n",
        "    parser.add_argument('--hide-labels', default=False, action='store_true', help='hide labels')\n",
        "    parser.add_argument('--hide-conf', default=False, action='store_true', help='hide confidences')\n",
        "    parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference')\n",
        "    parser.add_argument('--dnn', action='store_true', help='use OpenCV DNN for ONNX inference')\n",
        "    opt = parser.parse_args()\n",
        "    opt.imgsz *= 2 if len(opt.imgsz) == 1 else 1  # expand\n",
        "    print_args(FILE.stem, opt)\n",
        "    return opt\n",
        "\n",
        "\n",
        "def main(opt):\n",
        "    check_requirements(exclude=('tensorboard', 'thop'))\n",
        "    run(**vars(opt))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    opt = parse_opt()\n",
        "    main(opt)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw7rHYvsyqPB"
      },
      "source": [
        "# utils.plots.py ÍµêÏ≤¥"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrlgYIYoy9W3"
      },
      "source": [
        "# YOLOv5 üöÄ by Ultralytics, GPL-3.0 license\n",
        "\"\"\"\n",
        "Plotting utils\n",
        "\"\"\"\n",
        "\n",
        "# import sys\n",
        "# import io\n",
        "# sys.stdout = io.TextIOWrapper(sys.stdout.detach(), encoding = 'utf-8')\n",
        "# sys.stderr = io.TextIOWrapper(sys.stderr.detach(), encoding = 'utf-8')\n",
        "\n",
        "import math\n",
        "import os\n",
        "from copy import deepcopy\n",
        "from pathlib import Path\n",
        "\n",
        "import cv2\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "import torch\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "from utils.general import (LOGGER, Timeout, check_requirements, clip_coords, increment_path, is_ascii, is_chinese,\n",
        "                           try_except, user_config_dir, xywh2xyxy, xyxy2xywh)\n",
        "from utils.metrics import fitness\n",
        "\n",
        "# Settings\n",
        "CONFIG_DIR = user_config_dir()  # Ultralytics settings dir\n",
        "RANK = int(os.getenv('RANK', -1))\n",
        "matplotlib.rc('font', **{'size': 11})\n",
        "matplotlib.use('Agg')  # for writing to files only\n",
        "\n",
        "\n",
        "class Colors:\n",
        "    # Ultralytics color palette https://ultralytics.com/\n",
        "    def __init__(self):\n",
        "        # hex = matplotlib.colors.TABLEAU_COLORS.values()\n",
        "        hex = ('FF3838', 'FF9D97', 'FF701F', 'FFB21D', 'CFD231', '48F90A', '92CC17', '3DDB86', '1A9334', '00D4BB',\n",
        "               '2C99A8', '00C2FF', '344593', '6473FF', '0018EC', '8438FF', '520085', 'CB38FF', 'FF95C8', 'FF37C7')\n",
        "        self.palette = [self.hex2rgb('#' + c) for c in hex]\n",
        "        self.n = len(self.palette)\n",
        "\n",
        "    def __call__(self, i, bgr=False):\n",
        "        c = self.palette[int(i) % self.n]\n",
        "        return (c[2], c[1], c[0]) if bgr else c\n",
        "\n",
        "    @staticmethod\n",
        "    def hex2rgb(h):  # rgb order (PIL)\n",
        "        return tuple(int(h[1 + i:1 + i + 2], 16) for i in (0, 2, 4))\n",
        "\n",
        "\n",
        "colors = Colors()  # create instance for 'from utils.plots import colors'\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.font_manager as fm\n",
        "fm.get_fontconfig_fonts()\n",
        "font_location = '/content/yolov5/malgun.ttf'\n",
        "font_name = fm.FontProperties(fname=font_location)\n",
        "\n",
        "# matplotlib.rc('font', family=font_name)\n",
        "matplotlib.rcParams['axes.unicode_minus'] = False\n",
        "# Í∑∏ÎûòÌîÑÏóêÏÑú ÎßàÏù¥ÎÑàÏä§ Ìè∞Ìä∏ Íπ®ÏßÄÎäî Î¨∏Ï†úÏóê ÎåÄÌïú ÎåÄÏ≤ò\n",
        "\n",
        "######################### font='malgun.ttf'Î°ú ÍµêÏ≤¥ #############################################\n",
        "def check_font(font='malgun.ttf', size=20):\n",
        "    # Return a PIL TrueType Font, downloading to CONFIG_DIR if necessary\n",
        "    font = Path(font)\n",
        "    font = font if font.exists() else (CONFIG_DIR / font.name)\n",
        "    try:\n",
        "        return ImageFont.truetype(str(font) if font.exists() else font.name, size)\n",
        "    except Exception as e:  # download if missing\n",
        "        url = \"https://ultralytics.com/assets/\" + font.name\n",
        "        print(f'Downloading {url} to {font}...')\n",
        "        torch.hub.download_url_to_file(url, str(font), progress=False)\n",
        "        try:\n",
        "            return ImageFont.truetype(str(font), size)\n",
        "        except TypeError:\n",
        "            check_requirements('Pillow>=8.4.0')  # known issue https://github.com/ultralytics/yolov5/issues/5374\n",
        "\n",
        "\n",
        "class Annotator:\n",
        "    if RANK in (-1, 0):\n",
        "        check_font()  # download TTF if necessary\n",
        "\n",
        "    # YOLOv5 Annotator for train/val mosaics and jpgs and detect/hub inference annotations\n",
        "######################### font='malgun.ttf'Î°ú ÍµêÏ≤¥ #############################################\n",
        "    def __init__(self, im, line_width=None, font_size=None, font='malgun.ttf', pil=False, example='abc'):\n",
        "        assert im.data.contiguous, 'Image not contiguous. Apply np.ascontiguousarray(im) to Annotator() input images.'\n",
        "        self.pil = pil or not is_ascii(example) or is_chinese(example)\n",
        "        if self.pil:  # use PIL\n",
        "            self.im = im if isinstance(im, Image.Image) else Image.fromarray(im)\n",
        "            self.draw = ImageDraw.Draw(self.im)\n",
        "            self.font = check_font(font='Arial.Unicode.ttf' if is_chinese(example) else font,\n",
        "                                   size=font_size or max(round(sum(self.im.size) / 2 * 0.035), 12))\n",
        "        else:  # use cv2\n",
        "            self.im = im\n",
        "        self.lw = line_width or max(round(sum(im.shape) / 2 * 0.003), 2)  # line width\n",
        "\n",
        "    def box_label(self, box, label='', color=(128, 128, 128), txt_color=(255, 255, 255)):\n",
        "        # Add one xyxy box to image with label\n",
        "        if self.pil or not is_ascii(label):\n",
        "            self.draw.rectangle(box, width=self.lw, outline=color)  # box\n",
        "            if label:\n",
        "                w, h = self.font.getsize(label)  # text width, height\n",
        "                outside = box[1] - h >= 0  # label fits outside box\n",
        "                self.draw.rectangle([box[0],\n",
        "                                     box[1] - h if outside else box[1],\n",
        "                                     box[0] + w + 1,\n",
        "                                     box[1] + 1 if outside else box[1] + h + 1], fill=color)\n",
        "                # self.draw.text((box[0], box[1]), label, fill=txt_color, font=self.font, anchor='ls')  # for PIL>8.0\n",
        "                self.draw.text((box[0], box[1] - h if outside else box[1]), label, fill=txt_color, font=self.font)\n",
        "        else:  # cv2\n",
        "            p1, p2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))\n",
        "            cv2.rectangle(self.im, p1, p2, color, thickness=self.lw, lineType=cv2.LINE_AA)\n",
        "            if label:\n",
        "                tf = max(self.lw - 1, 1)  # font thickness\n",
        "                w, h = cv2.getTextSize(label, 0, fontScale=self.lw / 3, thickness=tf)[0]  # text width, height\n",
        "                outside = p1[1] - h - 3 >= 0  # label fits outside box\n",
        "                p2 = p1[0] + w, p1[1] - h - 3 if outside else p1[1] + h + 3\n",
        "                cv2.rectangle(self.im, p1, p2, color, -1, cv2.LINE_AA)  # filled\n",
        "                cv2.putText(self.im, label, (p1[0], p1[1] - 2 if outside else p1[1] + h + 2), 0, self.lw / 3, txt_color,\n",
        "                            thickness=tf, lineType=cv2.LINE_AA)\n",
        "\n",
        "    def rectangle(self, xy, fill=None, outline=None, width=1):\n",
        "        # Add rectangle to image (PIL-only)\n",
        "        self.draw.rectangle(xy, fill, outline, width)\n",
        "\n",
        "    def text(self, xy, text, txt_color=(255, 255, 255)):\n",
        "        # Add text to image (PIL-only)\n",
        "        w, h = self.font.getsize(text)  # text width, height\n",
        "        self.draw.text((xy[0], xy[1] - h + 1), text, fill=txt_color, font=self.font)\n",
        "\n",
        "    def result(self):\n",
        "        # Return annotated image as array\n",
        "        return np.asarray(self.im)\n",
        "\n",
        "\n",
        "def feature_visualization(x, module_type, stage, n=32, save_dir=Path('runs/detect/exp')):\n",
        "    \"\"\"\n",
        "    x:              Features to be visualized\n",
        "    module_type:    Module type\n",
        "    stage:          Module stage within model\n",
        "    n:              Maximum number of feature maps to plot\n",
        "    save_dir:       Directory to save results\n",
        "    \"\"\"\n",
        "    if 'Detect' not in module_type:\n",
        "        batch, channels, height, width = x.shape  # batch, channels, height, width\n",
        "        if height > 1 and width > 1:\n",
        "            f = save_dir / f\"stage{stage}_{module_type.split('.')[-1]}_features.png\"  # filename\n",
        "\n",
        "            blocks = torch.chunk(x[0].cpu(), channels, dim=0)  # select batch index 0, block by channels\n",
        "            n = min(n, channels)  # number of plots\n",
        "            fig, ax = plt.subplots(math.ceil(n / 8), 8, tight_layout=True)  # 8 rows x n/8 cols\n",
        "            ax = ax.ravel()\n",
        "            plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
        "            for i in range(n):\n",
        "                ax[i].imshow(blocks[i].squeeze())  # cmap='gray'\n",
        "                ax[i].axis('off')\n",
        "\n",
        "            print(f'Saving {f}... ({n}/{channels})')\n",
        "            plt.savefig(f, dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "            np.save(str(f.with_suffix('.npy')), x[0].cpu().numpy())  # npy save\n",
        "\n",
        "\n",
        "def hist2d(x, y, n=100):\n",
        "    # 2d histogram used in labels.png and evolve.png\n",
        "    xedges, yedges = np.linspace(x.min(), x.max(), n), np.linspace(y.min(), y.max(), n)\n",
        "    hist, xedges, yedges = np.histogram2d(x, y, (xedges, yedges))\n",
        "    xidx = np.clip(np.digitize(x, xedges) - 1, 0, hist.shape[0] - 1)\n",
        "    yidx = np.clip(np.digitize(y, yedges) - 1, 0, hist.shape[1] - 1)\n",
        "    return np.log(hist[xidx, yidx])\n",
        "\n",
        "\n",
        "def butter_lowpass_filtfilt(data, cutoff=1500, fs=50000, order=5):\n",
        "    from scipy.signal import butter, filtfilt\n",
        "\n",
        "    # https://stackoverflow.com/questions/28536191/how-to-filter-smooth-with-scipy-numpy\n",
        "    def butter_lowpass(cutoff, fs, order):\n",
        "        nyq = 0.5 * fs\n",
        "        normal_cutoff = cutoff / nyq\n",
        "        return butter(order, normal_cutoff, btype='low', analog=False)\n",
        "\n",
        "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
        "    return filtfilt(b, a, data)  # forward-backward filter\n",
        "\n",
        "\n",
        "def output_to_target(output):\n",
        "    # Convert model output to target format [batch_id, class_id, x, y, w, h, conf]\n",
        "    targets = []\n",
        "    for i, o in enumerate(output):\n",
        "        for *box, conf, cls in o.cpu().numpy():\n",
        "            targets.append([i, cls, *list(*xyxy2xywh(np.array(box)[None])), conf])\n",
        "    return np.array(targets)\n",
        "\n",
        "\n",
        "def plot_images(images, targets, paths=None, fname='images.jpg', names=None, max_size=1920, max_subplots=16):\n",
        "    # Plot image grid with labels\n",
        "    if isinstance(images, torch.Tensor):\n",
        "        images = images.cpu().float().numpy()\n",
        "    if isinstance(targets, torch.Tensor):\n",
        "        targets = targets.cpu().numpy()\n",
        "    if np.max(images[0]) <= 1:\n",
        "        images *= 255  # de-normalise (optional)\n",
        "    bs, _, h, w = images.shape  # batch size, _, height, width\n",
        "    bs = min(bs, max_subplots)  # limit plot images\n",
        "    ns = np.ceil(bs ** 0.5)  # number of subplots (square)\n",
        "\n",
        "    # Build Image\n",
        "    mosaic = np.full((int(ns * h), int(ns * w), 3), 255, dtype=np.uint8)  # init\n",
        "    for i, im in enumerate(images):\n",
        "        if i == max_subplots:  # if last batch has fewer images than we expect\n",
        "            break\n",
        "        x, y = int(w * (i // ns)), int(h * (i % ns))  # block origin\n",
        "        im = im.transpose(1, 2, 0)\n",
        "        mosaic[y:y + h, x:x + w, :] = im\n",
        "\n",
        "    # Resize (optional)\n",
        "    scale = max_size / ns / max(h, w)\n",
        "    if scale < 1:\n",
        "        h = math.ceil(scale * h)\n",
        "        w = math.ceil(scale * w)\n",
        "        mosaic = cv2.resize(mosaic, tuple(int(x * ns) for x in (w, h)))\n",
        "\n",
        "    # Annotate\n",
        "    fs = int((h + w) * ns * 0.01)  # font size\n",
        "    annotator = Annotator(mosaic, line_width=round(fs / 10), font_size=fs, pil=True)\n",
        "    for i in range(i + 1):\n",
        "        x, y = int(w * (i // ns)), int(h * (i % ns))  # block origin\n",
        "        annotator.rectangle([x, y, x + w, y + h], None, (255, 255, 255), width=2)  # borders\n",
        "        if paths:\n",
        "            annotator.text((x + 5, y + 5 + h), text=Path(paths[i]).name[:40], txt_color=(220, 220, 220))  # filenames\n",
        "        if len(targets) > 0:\n",
        "            ti = targets[targets[:, 0] == i]  # image targets\n",
        "            boxes = xywh2xyxy(ti[:, 2:6]).T\n",
        "            classes = ti[:, 1].astype('int')\n",
        "            labels = ti.shape[1] == 6  # labels if no conf column\n",
        "            conf = None if labels else ti[:, 6]  # check for confidence presence (label vs pred)\n",
        "\n",
        "            if boxes.shape[1]:\n",
        "                if boxes.max() <= 1.01:  # if normalized with tolerance 0.01\n",
        "                    boxes[[0, 2]] *= w  # scale to pixels\n",
        "                    boxes[[1, 3]] *= h\n",
        "                elif scale < 1:  # absolute coords need scale if image scales\n",
        "                    boxes *= scale\n",
        "            boxes[[0, 2]] += x\n",
        "            boxes[[1, 3]] += y\n",
        "            for j, box in enumerate(boxes.T.tolist()):\n",
        "                cls = classes[j]\n",
        "                color = colors(cls)\n",
        "                cls = names[cls] if names else cls\n",
        "                if labels or conf[j] > 0.25:  # 0.25 conf thresh\n",
        "                    label = f'{cls}' if labels else f'{cls} {conf[j]:.1f}'\n",
        "                    annotator.box_label(box, label, color=color)\n",
        "    annotator.im.save(fname)  # save\n",
        "\n",
        "\n",
        "def plot_lr_scheduler(optimizer, scheduler, epochs=300, save_dir=''):\n",
        "    # Plot LR simulating training for full epochs\n",
        "    optimizer, scheduler = deepcopy(optimizer), deepcopy(scheduler)  # do not modify originals\n",
        "    y = []\n",
        "    for _ in range(epochs):\n",
        "        scheduler.step()\n",
        "        y.append(optimizer.param_groups[0]['lr'])\n",
        "    plt.plot(y, '.-', label='LR')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('LR')\n",
        "    plt.grid()\n",
        "    plt.xlim(0, epochs)\n",
        "    plt.ylim(0)\n",
        "    plt.savefig(Path(save_dir) / 'LR.png', dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_val_txt():  # from utils.plots import *; plot_val()\n",
        "    # Plot val.txt histograms\n",
        "    x = np.loadtxt('val.txt', dtype=np.float32)\n",
        "    box = xyxy2xywh(x[:, :4])\n",
        "    cx, cy = box[:, 0], box[:, 1]\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(6, 6), tight_layout=True)\n",
        "    ax.hist2d(cx, cy, bins=600, cmax=10, cmin=0)\n",
        "    ax.set_aspect('equal')\n",
        "    plt.savefig('hist2d.png', dpi=300)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(12, 6), tight_layout=True)\n",
        "    ax[0].hist(cx, bins=600)\n",
        "    ax[1].hist(cy, bins=600)\n",
        "    plt.savefig('hist1d.png', dpi=200)\n",
        "\n",
        "\n",
        "def plot_targets_txt():  # from utils.plots import *; plot_targets_txt()\n",
        "    # Plot targets.txt histograms\n",
        "    x = np.loadtxt('targets.txt', dtype=np.float32).T\n",
        "    s = ['x targets', 'y targets', 'width targets', 'height targets']\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(8, 8), tight_layout=True)\n",
        "    ax = ax.ravel()\n",
        "    for i in range(4):\n",
        "        ax[i].hist(x[i], bins=100, label=f'{x[i].mean():.3g} +/- {x[i].std():.3g}')\n",
        "        ax[i].legend()\n",
        "        ax[i].set_title(s[i])\n",
        "    plt.savefig('targets.jpg', dpi=200)\n",
        "\n",
        "\n",
        "def plot_val_study(file='', dir='', x=None):  # from utils.plots import *; plot_val_study()\n",
        "    # Plot file=study.txt generated by val.py (or plot all study*.txt in dir)\n",
        "    save_dir = Path(file).parent if file else Path(dir)\n",
        "    plot2 = False  # plot additional results\n",
        "    if plot2:\n",
        "        ax = plt.subplots(2, 4, figsize=(10, 6), tight_layout=True)[1].ravel()\n",
        "\n",
        "    fig2, ax2 = plt.subplots(1, 1, figsize=(8, 4), tight_layout=True)\n",
        "    # for f in [save_dir / f'study_coco_{x}.txt' for x in ['yolov5n6', 'yolov5s6', 'yolov5m6', 'yolov5l6', 'yolov5x6']]:\n",
        "    for f in sorted(save_dir.glob('study*.txt')):\n",
        "        y = np.loadtxt(f, dtype=np.float32, usecols=[0, 1, 2, 3, 7, 8, 9], ndmin=2).T\n",
        "        x = np.arange(y.shape[1]) if x is None else np.array(x)\n",
        "        if plot2:\n",
        "            s = ['P', 'R', 'mAP@.5', 'mAP@.5:.95', 't_preprocess (ms/img)', 't_inference (ms/img)', 't_NMS (ms/img)']\n",
        "            for i in range(7):\n",
        "                ax[i].plot(x, y[i], '.-', linewidth=2, markersize=8)\n",
        "                ax[i].set_title(s[i])\n",
        "\n",
        "        j = y[3].argmax() + 1\n",
        "        ax2.plot(y[5, 1:j], y[3, 1:j] * 1E2, '.-', linewidth=2, markersize=8,\n",
        "                 label=f.stem.replace('study_coco_', '').replace('yolo', 'YOLO'))\n",
        "\n",
        "    ax2.plot(1E3 / np.array([209, 140, 97, 58, 35, 18]), [34.6, 40.5, 43.0, 47.5, 49.7, 51.5],\n",
        "             'k.-', linewidth=2, markersize=8, alpha=.25, label='EfficientDet')\n",
        "\n",
        "    ax2.grid(alpha=0.2)\n",
        "    ax2.set_yticks(np.arange(20, 60, 5))\n",
        "    ax2.set_xlim(0, 57)\n",
        "    ax2.set_ylim(25, 55)\n",
        "    ax2.set_xlabel('GPU Speed (ms/img)')\n",
        "    ax2.set_ylabel('COCO AP val')\n",
        "    ax2.legend(loc='lower right')\n",
        "    f = save_dir / 'study.png'\n",
        "    print(f'Saving {f}...')\n",
        "    plt.savefig(f, dpi=300)\n",
        "\n",
        "\n",
        "@try_except  # known issue https://github.com/ultralytics/yolov5/issues/5395\n",
        "@Timeout(30)  # known issue https://github.com/ultralytics/yolov5/issues/5611\n",
        "def plot_labels(labels, names=(), save_dir=Path('')):\n",
        "    # plot dataset labels\n",
        "    LOGGER.info(f\"Plotting labels to {save_dir / 'labels.jpg'}... \")\n",
        "    c, b = labels[:, 0], labels[:, 1:].transpose()  # classes, boxes\n",
        "    nc = int(c.max() + 1)  # number of classes\n",
        "    x = pd.DataFrame(b.transpose(), columns=['x', 'y', 'width', 'height'])\n",
        "\n",
        "    # seaborn correlogram\n",
        "    sn.pairplot(x, corner=True, diag_kind='auto', kind='hist', diag_kws=dict(bins=50), plot_kws=dict(pmax=0.9))\n",
        "    plt.savefig(save_dir / 'labels_correlogram.jpg', dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "    # matplotlib labels\n",
        "    matplotlib.use('svg')  # faster\n",
        "    ax = plt.subplots(2, 2, figsize=(8, 8), tight_layout=True)[1].ravel()\n",
        "    y = ax[0].hist(c, bins=np.linspace(0, nc, nc + 1) - 0.5, rwidth=0.8)\n",
        "    # [y[2].patches[i].set_color([x / 255 for x in colors(i)]) for i in range(nc)]  # update colors bug #3195\n",
        "    ax[0].set_ylabel('instances')\n",
        "    if 0 < len(names) < 30:\n",
        "        ax[0].set_xticks(range(len(names)))\n",
        "        ax[0].set_xticklabels(names, rotation=90, fontsize=10)\n",
        "    else:\n",
        "        ax[0].set_xlabel('classes')\n",
        "    sn.histplot(x, x='x', y='y', ax=ax[2], bins=50, pmax=0.9)\n",
        "    sn.histplot(x, x='width', y='height', ax=ax[3], bins=50, pmax=0.9)\n",
        "\n",
        "    # rectangles\n",
        "    labels[:, 1:3] = 0.5  # center\n",
        "    labels[:, 1:] = xywh2xyxy(labels[:, 1:]) * 2000\n",
        "    img = Image.fromarray(np.ones((2000, 2000, 3), dtype=np.uint8) * 255)\n",
        "    for cls, *box in labels[:1000]:\n",
        "        ImageDraw.Draw(img).rectangle(box, width=1, outline=colors(cls))  # plot\n",
        "    ax[1].imshow(img)\n",
        "    ax[1].axis('off')\n",
        "\n",
        "    for a in [0, 1, 2, 3]:\n",
        "        for s in ['top', 'right', 'left', 'bottom']:\n",
        "            ax[a].spines[s].set_visible(False)\n",
        "\n",
        "    plt.savefig(save_dir / 'labels.jpg', dpi=200)\n",
        "    matplotlib.use('Agg')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_evolve(evolve_csv='path/to/evolve.csv'):  # from utils.plots import *; plot_evolve()\n",
        "    # Plot evolve.csv hyp evolution results\n",
        "    evolve_csv = Path(evolve_csv)\n",
        "    data = pd.read_csv(evolve_csv)\n",
        "    keys = [x.strip() for x in data.columns]\n",
        "    x = data.values\n",
        "    f = fitness(x)\n",
        "    j = np.argmax(f)  # max fitness index\n",
        "    plt.figure(figsize=(10, 12), tight_layout=True)\n",
        "    matplotlib.rc('font', **{'size': 8})\n",
        "    for i, k in enumerate(keys[7:]):\n",
        "        v = x[:, 7 + i]\n",
        "        mu = v[j]  # best single result\n",
        "        plt.subplot(6, 5, i + 1)\n",
        "        plt.scatter(v, f, c=hist2d(v, f, 20), cmap='viridis', alpha=.8, edgecolors='none')\n",
        "        plt.plot(mu, f.max(), 'k+', markersize=15)\n",
        "        plt.title(f'{k} = {mu:.3g}', fontdict={'size': 9})  # limit to 40 characters\n",
        "        if i % 5 != 0:\n",
        "            plt.yticks([])\n",
        "        print(f'{k:>15}: {mu:.3g}')\n",
        "    f = evolve_csv.with_suffix('.png')  # filename\n",
        "    plt.savefig(f, dpi=200)\n",
        "    plt.close()\n",
        "    print(f'Saved {f}')\n",
        "\n",
        "\n",
        "def plot_results(file='path/to/results.csv', dir=''):\n",
        "    # Plot training results.csv. Usage: from utils.plots import *; plot_results('path/to/results.csv')\n",
        "    save_dir = Path(file).parent if file else Path(dir)\n",
        "    fig, ax = plt.subplots(2, 5, figsize=(12, 6), tight_layout=True)\n",
        "    ax = ax.ravel()\n",
        "    files = list(save_dir.glob('results*.csv'))\n",
        "    assert len(files), f'No results.csv files found in {save_dir.resolve()}, nothing to plot.'\n",
        "    for fi, f in enumerate(files):\n",
        "        try:\n",
        "            data = pd.read_csv(f)\n",
        "            s = [x.strip() for x in data.columns]\n",
        "            x = data.values[:, 0]\n",
        "            for i, j in enumerate([1, 2, 3, 4, 5, 8, 9, 10, 6, 7]):\n",
        "                y = data.values[:, j]\n",
        "                # y[y == 0] = np.nan  # don't show zero values\n",
        "                ax[i].plot(x, y, marker='.', label=f.stem, linewidth=2, markersize=8)\n",
        "                ax[i].set_title(s[j], fontsize=12)\n",
        "                # if j in [8, 9, 10]:  # share train and val loss y axes\n",
        "                #     ax[i].get_shared_y_axes().join(ax[i], ax[i - 5])\n",
        "        except Exception as e:\n",
        "            print(f'Warning: Plotting error for {f}: {e}')\n",
        "    ax[1].legend()\n",
        "    fig.savefig(save_dir / 'results.png', dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def profile_idetection(start=0, stop=0, labels=(), save_dir=''):\n",
        "    # Plot iDetection '*.txt' per-image logs. from utils.plots import *; profile_idetection()\n",
        "    ax = plt.subplots(2, 4, figsize=(12, 6), tight_layout=True)[1].ravel()\n",
        "    s = ['Images', 'Free Storage (GB)', 'RAM Usage (GB)', 'Battery', 'dt_raw (ms)', 'dt_smooth (ms)', 'real-world FPS']\n",
        "    files = list(Path(save_dir).glob('frames*.txt'))\n",
        "    for fi, f in enumerate(files):\n",
        "        try:\n",
        "            results = np.loadtxt(f, ndmin=2).T[:, 90:-30]  # clip first and last rows\n",
        "            n = results.shape[1]  # number of rows\n",
        "            x = np.arange(start, min(stop, n) if stop else n)\n",
        "            results = results[:, x]\n",
        "            t = (results[0] - results[0].min())  # set t0=0s\n",
        "            results[0] = x\n",
        "            for i, a in enumerate(ax):\n",
        "                if i < len(results):\n",
        "                    label = labels[fi] if len(labels) else f.stem.replace('frames_', '')\n",
        "                    a.plot(t, results[i], marker='.', label=label, linewidth=1, markersize=5)\n",
        "                    a.set_title(s[i])\n",
        "                    a.set_xlabel('time (s)')\n",
        "                    # if fi == len(files) - 1:\n",
        "                    #     a.set_ylim(bottom=0)\n",
        "                    for side in ['top', 'right']:\n",
        "                        a.spines[side].set_visible(False)\n",
        "                else:\n",
        "                    a.remove()\n",
        "        except Exception as e:\n",
        "            print(f'Warning: Plotting error for {f}; {e}')\n",
        "    ax[1].legend()\n",
        "    plt.savefig(Path(save_dir) / 'idetection_profile.png', dpi=200)\n",
        "\n",
        "\n",
        "def save_one_box(xyxy, im, file='image.jpg', gain=1.02, pad=10, square=False, BGR=False, save=True):\n",
        "    # Save image crop as {file} with crop size multiple {gain} and {pad} pixels. Save and/or return crop\n",
        "    xyxy = torch.tensor(xyxy).view(-1, 4)\n",
        "    b = xyxy2xywh(xyxy)  # boxes\n",
        "    if square:\n",
        "        b[:, 2:] = b[:, 2:].max(1)[0].unsqueeze(1)  # attempt rectangle to square\n",
        "    b[:, 2:] = b[:, 2:] * gain + pad  # box wh * gain + pad\n",
        "    xyxy = xywh2xyxy(b).long()\n",
        "    clip_coords(xyxy, im.shape)\n",
        "    crop = im[int(xyxy[0, 1]):int(xyxy[0, 3]), int(xyxy[0, 0]):int(xyxy[0, 2]), ::(1 if BGR else -1)]\n",
        "    if save:\n",
        "        file.parent.mkdir(parents=True, exist_ok=True)  # make directory\n",
        "        cv2.imwrite(str(increment_path(file).with_suffix('.jpg')), crop)\n",
        "    return crop\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "OjRp16ct8kUv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzxaLKiybwkY",
        "outputId": "1a08f3ea-cf5f-429f-dc2b-71bd1423def8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python detect.py --weights /content/drive/MyDrive/okra/food689_best_ep1000.pt --img 640 --conf 0.25 --source /content/drive/MyDrive/okra/test/image5.jpg --device 0 --project /content/drive/MyDrive/okra/result --line-thickness 10"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/drive/MyDrive/okra/food689_best_ep1000.pt'], source=/content/drive/MyDrive/okra/test/image5.jpg, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=0, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=/content/drive/MyDrive/okra/result, name=exp, exist_ok=False, line_thickness=10, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
            "YOLOv5 üöÄ v6.0-127-g554f782 torch 1.10.0+cu111 CUDA:0 (Tesla K80, 11441MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 484 layers, 93228765 parameters, 0 gradients\n",
            "image 1/1 /content/drive/MyDrive/okra/test/image5.jpg: 480x640 1 Ïò§Ïù¥Ï¥àÏ†àÏûÑ, Done. (0.292s)\n",
            "Speed: 0.7ms pre-process, 291.7ms inference, 2.0ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/okra/result/exp15\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4nvw71xbwfM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_gAUKsmbwck"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}