{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "211104_689종_목록.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/K-107/dki/blob/main/Food_Detection/211207_YOLOv5_%EB%8F%99%EC%98%81%EC%83%81.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ON-gSgXCag1y",
        "outputId": "b277529d-e007-4a3b-b944-ed2de4477aa8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8chNnKmadOq",
        "outputId": "dd4e31b9-047f-464d-885d-ae67e64c1506",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install openpyxl"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.7/dist-packages (2.5.9)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: jdcal in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTYmRYH8cd9b",
        "outputId": "3e110b29-1e71-45d5-c791-0afe4253f27c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, glob\n",
        "import shutil\n",
        "\n",
        "df = pd.read_excel('/content/drive/MyDrive/okra/조리음식목록 및 현황.xlsx', engine='openpyxl') # 엑셀 파일 로드\n",
        "df = df.iloc[:690,2]\n",
        "df.dropna(inplace=True) # 널값 제거\n",
        "label = df.to_list()\n",
        "label"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['가리비구이',\n",
              " '가자미구이',\n",
              " '가자미조림',\n",
              " '가지나물',\n",
              " '가지냉국',\n",
              " '가지볶음',\n",
              " '가지전',\n",
              " '가지찜',\n",
              " '감',\n",
              " '개불',\n",
              " '거봉',\n",
              " '게살죽',\n",
              " '게찜',\n",
              " '고구마',\n",
              " '고래고기',\n",
              " '고사리나물',\n",
              " '고사리볶음',\n",
              " '고추잡채',\n",
              " '곤약조림',\n",
              " '골뱅이무침',\n",
              " '곱창볶음',\n",
              " '곶감',\n",
              " '과일샐러드',\n",
              " '구아바',\n",
              " '구절판',\n",
              " '구절판',\n",
              " '굴전',\n",
              " '귀리밥',\n",
              " '귤',\n",
              " '금귤',\n",
              " '김',\n",
              " '김치볶음',\n",
              " '깐풍기',\n",
              " '깨',\n",
              " '깻잎나물',\n",
              " '깻잎나물볶음',\n",
              " '깻잎전',\n",
              " '깻잎찜',\n",
              " '꽈리고추찜',\n",
              " '낙지볶음',\n",
              " '난자완스',\n",
              " '노각무침',\n",
              " '녹두전',\n",
              " '느타리버섯볶음',\n",
              " '다래',\n",
              " '단감',\n",
              " '단무지',\n",
              " '단무지무침',\n",
              " '단팥죽',\n",
              " '닭고기냉채',\n",
              " '닭고기샐러드',\n",
              " '대게(영덕게)찜',\n",
              " '대구지리',\n",
              " '대구찜',\n",
              " '대추',\n",
              " '대하구이',\n",
              " '도가니탕',\n",
              " '도넛',\n",
              " '도라지나물',\n",
              " '도라지볶음',\n",
              " '도루묵조림',\n",
              " '도토리묵무침',\n",
              " '돼지갈비찜',\n",
              " '두릅',\n",
              " '두릅튀김',\n",
              " '두부양념조림',\n",
              " '딸기',\n",
              " '떡',\n",
              " '라조기',\n",
              " '리치',\n",
              " '마늘장아찌',\n",
              " '마늘쫑볶음',\n",
              " '마카다미아',\n",
              " '마파두부',\n",
              " '갈비구이',\n",
              " '갈비찜',\n",
              " '갈비탕',\n",
              " '갈치구이',\n",
              " '감자전',\n",
              " '감자조림',\n",
              " '감자탕',\n",
              " '갓김치',\n",
              " '고등어구이',\n",
              " '고등어조림',\n",
              " '김치전',\n",
              " '김치찌개',\n",
              " '깍두기',\n",
              " '깻잎장아찌',\n",
              " '꼬막찜',\n",
              " '꽁치조림',\n",
              " '나박김치',\n",
              " '닭계장',\n",
              " '닭볶음탕',\n",
              " '더덕구이',\n",
              " '도라지무침',\n",
              " '동태찌개',\n",
              " '된장찌개',\n",
              " '라면',\n",
              " '만두국',\n",
              " '맑은국',\n",
              " '맛탕',\n",
              " '망고',\n",
              " '매생이국',\n",
              " '매생이죽',\n",
              " '매운탕',\n",
              " '머위나물',\n",
              " '메밀국수',\n",
              " '멸치볶음',\n",
              " '모과',\n",
              " '몸국',\n",
              " '무국',\n",
              " '무나물',\n",
              " '무말랭이무침',\n",
              " '무생채',\n",
              " '무지개송어구이',\n",
              " '무침',\n",
              " '무화과',\n",
              " '문어',\n",
              " '물냉면',\n",
              " '물회',\n",
              " '미나리강회',\n",
              " '미역국',\n",
              " '미역냉국',\n",
              " '미역줄기볶음',\n",
              " '바게트',\n",
              " '바다가재(랍스터)',\n",
              " '바지락찜',\n",
              " '방울토마토',\n",
              " '방풍나물',\n",
              " '배추김치',\n",
              " '배추전',\n",
              " '백김치',\n",
              " '버섯볶음',\n",
              " '버섯전골',\n",
              " '버섯튀김',\n",
              " '볶음밥',\n",
              " '부리토',\n",
              " '부추김치',\n",
              " '부추전',\n",
              " '부침',\n",
              " '브라질너트',\n",
              " '브로콜리',\n",
              " '비빔냉면',\n",
              " '빵',\n",
              " '사과',\n",
              " '삼계탕',\n",
              " '삼치구이',\n",
              " '새우',\n",
              " '새우튀김',\n",
              " '생선구이',\n",
              " '소갈비찜',\n",
              " '소고기덮밥',\n",
              " '수박',\n",
              " '순두부찌개',\n",
              " '스카치에그',\n",
              " '스크램블드에그',\n",
              " '식빵',\n",
              " '식빵튀김',\n",
              " '아구찜',\n",
              " '아보카도',\n",
              " '아스파라거스',\n",
              " '약식',\n",
              " '양념게장',\n",
              " '양념치킨',\n",
              " '양다리살구이',\n",
              " '양파',\n",
              " '어묵국',\n",
              " '어묵조림',\n",
              " '어묵튀김',\n",
              " '연근조림',\n",
              " '연근튀김',\n",
              " '연두부',\n",
              " '연어덮밥',\n",
              " '연잎밥',\n",
              " '열무국수',\n",
              " '열무김치',\n",
              " '영양돌솥밥',\n",
              " '오리백숙',\n",
              " '오리알',\n",
              " '오이',\n",
              " '오이냉국',\n",
              " '오이소박이',\n",
              " '오이양파무침',\n",
              " '오징어구이',\n",
              " '오징어덮밥',\n",
              " '오징어볶음',\n",
              " '오징어순대',\n",
              " '오징어조림',\n",
              " '오징어채무침',\n",
              " '오징어튀김',\n",
              " '옥수수',\n",
              " '용과',\n",
              " '우엉',\n",
              " '우엉조림',\n",
              " '육개장',\n",
              " '임연수구이',\n",
              " '잡채밥',\n",
              " '장어양념구이',\n",
              " '장어튀김',\n",
              " '전갱이튀김',\n",
              " '전복',\n",
              " '젓갈',\n",
              " '정어리구이',\n",
              " '조개구이',\n",
              " '조기구이',\n",
              " '죽순볶음',\n",
              " '짜장면',\n",
              " '짜장밥',\n",
              " '짬뽕',\n",
              " '참나물무침',\n",
              " '참치회',\n",
              " '체리',\n",
              " '초코케이크',\n",
              " '총각김치',\n",
              " '추어탕',\n",
              " '칠면조구이',\n",
              " '카레라이스',\n",
              " '캐슈넛',\n",
              " '컵케이크',\n",
              " '케일',\n",
              " '코올슬로',\n",
              " '콜라비',\n",
              " '콩국수',\n",
              " '콩나물국',\n",
              " '콩조림',\n",
              " '크림스프',\n",
              " '키위',\n",
              " '탕수육',\n",
              " '탕평채',\n",
              " '토마토',\n",
              " '토마토스크램블',\n",
              " '파강회',\n",
              " '파김치',\n",
              " '파래무침',\n",
              " '파운드케이크',\n",
              " '파파야',\n",
              " '패션후르츠',\n",
              " '편육',\n",
              " '포도',\n",
              " '푸딩',\n",
              " '피스타치오',\n",
              " '피칸',\n",
              " '한라봉',\n",
              " '함박스테이크정식',\n",
              " '해물덮밥',\n",
              " '해물찜',\n",
              " '해물탕',\n",
              " '해바라기씨',\n",
              " '해삼',\n",
              " '해쉬브라운포테이토',\n",
              " '해파리냉채',\n",
              " '호두',\n",
              " '호박씨',\n",
              " '화양적',\n",
              " '황태구이',\n",
              " '후라이드치킨',\n",
              " '훈제오리',\n",
              " '흰죽',\n",
              " '쌀밥',\n",
              " '비빔밥',\n",
              " '라멘',\n",
              " '된장국',\n",
              " '만두',\n",
              " '쌀국수',\n",
              " '불고기',\n",
              " '삼겹살구이',\n",
              " '감자채볶음',\n",
              " '건새우볶음',\n",
              " '고추튀김',\n",
              " '곱창전골',\n",
              " '김밥',\n",
              " '김치볶음밥',\n",
              " '꿀떡',\n",
              " '누룽지',\n",
              " '도토리묵',\n",
              " '두부조림',\n",
              " '땅콩조림',\n",
              " '떡볶이',\n",
              " '막국수',\n",
              " '멍게',\n",
              " '새우볶음밥',\n",
              " '송편',\n",
              " '수제비',\n",
              " '알밥',\n",
              " '유부초밥',\n",
              " '육회',\n",
              " '잔치국수',\n",
              " '잡곡밥',\n",
              " '잡채',\n",
              " '장조림',\n",
              " '전복죽',\n",
              " '제육볶음',\n",
              " '주꾸미볶음',\n",
              " '쫄면',\n",
              " '칼국수',\n",
              " '파전',\n",
              " '호박전',\n",
              " '호박죽',\n",
              " '순대',\n",
              " '계란후라이',\n",
              " '쇠고기카레라이스',\n",
              " '계란말이',\n",
              " '경단',\n",
              " '전통떡갈비',\n",
              " '동그랑땡',\n",
              " '두부김치',\n",
              " '수정과',\n",
              " '식혜',\n",
              " '약과',\n",
              " '간장게장',\n",
              " '생선초밥',\n",
              " '진미채볶음',\n",
              " '곰탕_설렁탕',\n",
              " '라볶이',\n",
              " '보쌈',\n",
              " '오징어채볶음',\n",
              " '주먹밥',\n",
              " '홍어회무침',\n",
              " '회무침',\n",
              " '돼지고기_수육',\n",
              " '시래기된장국',\n",
              " '닭갈비',\n",
              " '갈치조림',\n",
              " '어묵볶음',\n",
              " '김치찜',\n",
              " '샌드위치',\n",
              " '스튜',\n",
              " '스파게티',\n",
              " '애플파이',\n",
              " '팝콘',\n",
              " '필라프',\n",
              " '프렌치토스트',\n",
              " '베이글',\n",
              " '롤빵',\n",
              " '햄버거',\n",
              " '오트밀',\n",
              " '돈가스',\n",
              " '햄커틀렛',\n",
              " '치킨까스',\n",
              " '우동',\n",
              " '오믈렛',\n",
              " '나토',\n",
              " '감자샐러드',\n",
              " '오므라이스',\n",
              " '그라탕',\n",
              " '고로케',\n",
              " '생선회',\n",
              " '스콘',\n",
              " '크루아상',\n",
              " '마카로니샐러드',\n",
              " '시저샐러드',\n",
              " '너겟',\n",
              " '버섯리조또',\n",
              " '라자냐',\n",
              " '붕어빵',\n",
              " '닭꼬치',\n",
              " '츄러스',\n",
              " '브라우니',\n",
              " '갈치튀김',\n",
              " '굴',\n",
              " '두부튀김',\n",
              " '떡고치구이',\n",
              " '말린 사과',\n",
              " '배추나물',\n",
              " '소시지조림',\n",
              " '참치볶음',\n",
              " '치커리',\n",
              " '콩볶음',\n",
              " '팽이버섯무침',\n",
              " '호빵',\n",
              " '닭날개',\n",
              " '돈육두루치기',\n",
              " '돼지고기구이',\n",
              " '두부탕',\n",
              " '배추',\n",
              " '비트',\n",
              " '소고기볶음',\n",
              " '소시지야채볶음',\n",
              " '스프',\n",
              " '오렌지',\n",
              " '옥수수구이',\n",
              " '옥수수전',\n",
              " '은행',\n",
              " '자두',\n",
              " '쥐포구이',\n",
              " '닭죽',\n",
              " '야채죽',\n",
              " '옥수수밥',\n",
              " '콩나물밥',\n",
              " '무밥',\n",
              " '야채볶음',\n",
              " '오이볶음',\n",
              " '소고기장조림',\n",
              " '당근조림',\n",
              " '무조림',\n",
              " '양배추겉절이',\n",
              " '시금치나물',\n",
              " '풋고추조림',\n",
              " '미음',\n",
              " '닭다리',\n",
              " '들깻잎',\n",
              " '말린 바나나',\n",
              " '삶은 단호박',\n",
              " '순두부',\n",
              " '야채쌈',\n",
              " '오이초절임',\n",
              " '쪽파',\n",
              " '찰밥',\n",
              " '청경채',\n",
              " '북어국',\n",
              " '보리밥',\n",
              " '소고기국',\n",
              " '우엉볶음',\n",
              " '무숙채',\n",
              " '미역나물',\n",
              " '건자두',\n",
              " '건포도',\n",
              " '계란',\n",
              " '골드키위',\n",
              " '감자',\n",
              " '감자야채볶음',\n",
              " '당근',\n",
              " '두부부침',\n",
              " '두부전',\n",
              " '깻잎조림',\n",
              " '두부구이',\n",
              " '땅콩',\n",
              " '레몬',\n",
              " '마늘',\n",
              " '배',\n",
              " '복숭아',\n",
              " '상추',\n",
              " '새송이버섯',\n",
              " '생밤',\n",
              " '쌈무',\n",
              " '오이고추',\n",
              " '잣',\n",
              " '장아찌',\n",
              " '표고버섯',\n",
              " '풋고추',\n",
              " '야채샐러드',\n",
              " '돼지고기장조림',\n",
              " '부추겉절이',\n",
              " '양파겉절이',\n",
              " '숙주나물',\n",
              " '양파볶음',\n",
              " '호박볶음',\n",
              " '양배추찜',\n",
              " '버섯채소볶음',\n",
              " '감자어묵볶음',\n",
              " '닭가슴살',\n",
              " '계란(달걀)조림',\n",
              " '참외',\n",
              " '다시마조림',\n",
              " '감자수제비국',\n",
              " '김튀김',\n",
              " '완두콩밥',\n",
              " '취나물볶음',\n",
              " '고구마조림',\n",
              " '양배추나물',\n",
              " '강낭콩',\n",
              " '다시마',\n",
              " '호박잎',\n",
              " '삼각김밥',\n",
              " '컵라면',\n",
              " '호박잎찜',\n",
              " '초코파이(내용물)',\n",
              " '검정콩조림',\n",
              " '고구마줄기무침',\n",
              " '김치전골',\n",
              " '닭',\n",
              " '석류',\n",
              " '조개살',\n",
              " '들깨국',\n",
              " '우거지국',\n",
              " '쥐포조림',\n",
              " '떡강정',\n",
              " '고등어튀김',\n",
              " '굴무침',\n",
              " '은행밥',\n",
              " '순대국밥',\n",
              " '참치죽',\n",
              " '소고기버섯죽',\n",
              " '조개국',\n",
              " '돼지고기완자전',\n",
              " '멸치깻잎볶음',\n",
              " '피망볶음',\n",
              " '깻잎튀김',\n",
              " '쑥갓생채',\n",
              " '시래기나물',\n",
              " '쑥갓나물',\n",
              " '마늘쫑조림',\n",
              " '애호박찜',\n",
              " '연근전',\n",
              " '호박조림',\n",
              " '갑오징어',\n",
              " '개암',\n",
              " '곤약잡채',\n",
              " '까나리',\n",
              " '꼴뚜기',\n",
              " '꽃게탕',\n",
              " '낙지',\n",
              " '닭다리조림',\n",
              " '도가니수육',\n",
              " '돼지족발찜',\n",
              " '떡산적',\n",
              " '라임',\n",
              " '마',\n",
              " '마카로니케첩볶음',\n",
              " '베이컨말이떡구이',\n",
              " '부대고기찌개',\n",
              " '북어볶음',\n",
              " '불낙전골',\n",
              " '산딸기',\n",
              " '생강초절임',\n",
              " '샤브샤브',\n",
              " '소고기 수육',\n",
              " '소고기튀김',\n",
              " '애플망고',\n",
              " '엘더베리',\n",
              " '오리찜',\n",
              " '오이물김치',\n",
              " '우뭇가사리',\n",
              " '원추리',\n",
              " '월남쌈',\n",
              " '유산슬',\n",
              " '조개젓',\n",
              " '조개탕',\n",
              " '재첩국',\n",
              " '복지리',\n",
              " '오징어찜',\n",
              " '조기찜',\n",
              " '붕어찜',\n",
              " '더덕찜',\n",
              " '굴비구이',\n",
              " '볼락구이',\n",
              " '꽁치구이',\n",
              " '노가리구이',\n",
              " '병어구이',\n",
              " '전어구이',\n",
              " '참치구이',\n",
              " '청어구이',\n",
              " '코다리구이',\n",
              " '붕어구이',\n",
              " '전갱이구이(각재기구이)',\n",
              " '김치메밀전병',\n",
              " '뱅어포볶음',\n",
              " '자반고등어조림',\n",
              " '장어조림',\n",
              " '새우장',\n",
              " '대구튀김',\n",
              " '멸치튀김',\n",
              " '오징어탕수',\n",
              " '미꾸라지튀김',\n",
              " '메밀묵무침',\n",
              " '고비나물',\n",
              " '곤드레나물',\n",
              " '꼴뚜기무침',\n",
              " '낙지무침',\n",
              " '노가리채무침',\n",
              " '멸치잔파무침',\n",
              " '문어무침',\n",
              " '새우무침',\n",
              " '소라무침',\n",
              " '어리굴젓무침',\n",
              " '창란젓무무침',\n",
              " '피조개무침',\n",
              " '홍합무침',\n",
              " '뱅어포무침',\n",
              " '모자반무침',\n",
              " '사태초무침',\n",
              " '소고기김무침',\n",
              " '가자미찜',\n",
              " '고등어찜',\n",
              " '꽃게찜',\n",
              " '동태찜',\n",
              " '미더덕찜',\n",
              " '북어찜',\n",
              " '홍합찜',\n",
              " '명란찜',\n",
              " '코다리찜',\n",
              " '대구구이',\n",
              " '명태구이',\n",
              " '방어구이',\n",
              " '서대구이',\n",
              " '키조개구이',\n",
              " '밤팥밥',\n",
              " '참치회덮밥',\n",
              " '새우죽',\n",
              " '굴죽',\n",
              " '호박범벅',\n",
              " '해삼탕',\n",
              " '넙치구이',\n",
              " '민어구이',\n",
              " '오징어옥수수전',\n",
              " '전유어',\n",
              " '소고기완자전',\n",
              " '느타리산적',\n",
              " '쑥전',\n",
              " '오리불고기',\n",
              " '근대볶음',\n",
              " '무청볶음',\n",
              " '동태조림',\n",
              " '병어조림',\n",
              " '삼치조림',\n",
              " '전어조림',\n",
              " '홍합조림',\n",
              " '연어다시마조림',\n",
              " '돼지고기갈비강정',\n",
              " '돼지고기완자조림',\n",
              " '양송이버섯조림',\n",
              " '강남콩조림',\n",
              " '미역야채튀김',\n",
              " '미역튀각',\n",
              " '다시마매듭자반',\n",
              " '바나나탕수',\n",
              " '겨자채',\n",
              " '두부냉채',\n",
              " '톳나물',\n",
              " '죽순오이무침',\n",
              " '냉이나물',\n",
              " '미나리나물',\n",
              " '씀바귀나물',\n",
              " '어묵콩나물부추무침',\n",
              " '토란대무침',\n",
              " '원추리나물',\n",
              " '유채나물',\n",
              " '원추리무침',\n",
              " '세발나물무침',\n",
              " '뽕잎나물돼지고기무침',\n",
              " '고춧잎볶음',\n",
              " '두릅적',\n",
              " '미역줄기잡채',\n",
              " '버섯찜',\n",
              " '부추강회',\n",
              " '완두콩조림',\n",
              " '우엉어묵조림',\n",
              " '유부조림',\n",
              " '취나물전',\n",
              " '토란조림',\n",
              " '머위잎',\n",
              " '멜론',\n",
              " '양갈비구이',\n",
              " '오징어무침',\n",
              " '해물파전',\n",
              " '대구포무침',\n",
              " '북어채무침',\n",
              " '불고기덮밥',\n",
              " '대합죽',\n",
              " '고둥조림',\n",
              " '풋고추튀김',\n",
              " '비름나물',\n",
              " '취나물',\n",
              " '가죽나물무침',\n",
              " '머루',\n",
              " '복분자(생과)',\n",
              " '홍어찜',\n",
              " '황새치구이',\n",
              " '앵두',\n",
              " '보리새우찜',\n",
              " '누에동충하초',\n",
              " '무',\n",
              " '미트볼',\n",
              " '바나나',\n",
              " '보신탕',\n",
              " '부추잡채',\n",
              " '살구',\n",
              " '생선가스',\n",
              " '아몬드',\n",
              " '양배추',\n",
              " '울외장아찌(나라즈케)',\n",
              " '잉어찜',\n",
              " '자몽',\n",
              " '코코넛',\n",
              " '크린베리',\n",
              " '파인애플',\n",
              " '파프리카',\n",
              " '페이스트리',\n",
              " '현미밥',\n",
              " '흑미밥',\n",
              " '계란찜',\n",
              " '미역',\n",
              " '선짓국',\n",
              " '소라회',\n",
              " '양상추',\n",
              " '햄구이',\n",
              " '블루베리',\n",
              " '파래야채튀김',\n",
              " '콩나물무침',\n",
              " '소고기 구이']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUJzMoFQbwml",
        "outputId": "73cd8fb6-165d-4700-c6b3-871296d670a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt  # install\n",
        "\n",
        "from yolov5 import utils\n",
        "display = utils.notebook_init()  # checks"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v6.0-127-g554f782 torch 1.10.0+cu111 CUDA:0 (Tesla K80, 11441MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://www.wfonts.com/download/data/2016/06/13/malgun-gothic/malgun.ttf\""
      ],
      "metadata": {
        "id": "OwLw9zpXjRr4",
        "outputId": "d6501ac1-2b49-4f4c-e0be-13cd628d3602",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-08 00:59:09--  https://www.wfonts.com/download/data/2016/06/13/malgun-gothic/malgun.ttf\n",
            "Resolving www.wfonts.com (www.wfonts.com)... 104.225.219.210\n",
            "Connecting to www.wfonts.com (www.wfonts.com)|104.225.219.210|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9595100 (9.2M) [application/octetstream]\n",
            "Saving to: ‘malgun.ttf’\n",
            "\n",
            "malgun.ttf          100%[===================>]   9.15M  5.36MB/s    in 1.7s    \n",
            "\n",
            "2021-12-08 00:59:11 (5.36 MB/s) - ‘malgun.ttf’ saved [9595100/9595100]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDRzrmCWrpYX",
        "outputId": "3a7bd14f-bd9c-4e49-cc8d-1c3c57c6409d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDWChPJdyhol"
      },
      "source": [
        "# 1. detec.py 파일 교체"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-BjBcvuy5Lm"
      },
      "source": [
        "# YOLOv5 🚀 by Ultralytics, GPL-3.0 license\n",
        "\"\"\"\n",
        "Run inference on images, videos, directories, streams, etc.\n",
        "\n",
        "Usage:\n",
        "    $ python path/to/detect.py --weights yolov5s.pt --source 0  # webcam\n",
        "                                                             img.jpg  # image\n",
        "                                                             vid.mp4  # video\n",
        "                                                             path/  # directory\n",
        "                                                             path/*.jpg  # glob\n",
        "                                                             'https://youtu.be/Zgi9g1ksQHc'  # YouTube\n",
        "                                                             'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, glob\n",
        "import shutil\n",
        "\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "FILE = Path(__file__).resolve()\n",
        "ROOT = FILE.parents[0]  # YOLOv5 root directory\n",
        "if str(ROOT) not in sys.path:\n",
        "    sys.path.append(str(ROOT))  # add ROOT to PATH\n",
        "ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n",
        "\n",
        "from models.common import DetectMultiBackend\n",
        "from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def run(weights=ROOT / 'yolov5s.pt',  # model.pt path(s)\n",
        "        source=ROOT / 'data/images',  # file/dir/URL/glob, 0 for webcam\n",
        "        imgsz=640,  # inference size (pixels)\n",
        "        conf_thres=0.25,  # confidence threshold\n",
        "        iou_thres=0.45,  # NMS IOU threshold\n",
        "        max_det=1000,  # maximum detections per image\n",
        "        device='',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
        "        view_img=False,  # show results\n",
        "        save_txt=False,  # save results to *.txt\n",
        "        save_conf=False,  # save confidences in --save-txt labels\n",
        "        save_crop=False,  # save cropped prediction boxes\n",
        "        nosave=False,  # do not save images/videos\n",
        "        classes=None,  # filter by class: --class 0, or --class 0 2 3\n",
        "        agnostic_nms=False,  # class-agnostic NMS\n",
        "        augment=False,  # augmented inference\n",
        "        visualize=False,  # visualize features\n",
        "        update=False,  # update all models\n",
        "        project=ROOT / 'runs/detect',  # save results to project/name\n",
        "        name='exp',  # save results to project/name\n",
        "        exist_ok=False,  # existing project/name ok, do not increment\n",
        "        line_thickness=3,  # bounding box thickness (pixels)\n",
        "        hide_labels=False,  # hide labels\n",
        "        hide_conf=False,  # hide confidences\n",
        "        half=False,  # use FP16 half-precision inference\n",
        "        dnn=False,  # use OpenCV DNN for ONNX inference\n",
        "        ):\n",
        "    source = str(source)\n",
        "    save_img = not nosave and not source.endswith('.txt')  # save inference images\n",
        "    is_file = Path(source).suffix[1:] in (IMG_FORMATS + VID_FORMATS)\n",
        "    is_url = source.lower().startswith(('rtsp://', 'rtmp://', 'http://', 'https://'))\n",
        "    webcam = source.isnumeric() or source.endswith('.txt') or (is_url and not is_file)\n",
        "    if is_url and is_file:\n",
        "        source = check_file(source)  # download\n",
        "\n",
        "    # Directories\n",
        "    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\n",
        "    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
        "\n",
        "    # Load model\n",
        "################################ model.names 삭제 및 교체 ###############################################\n",
        "    df = pd.read_excel('/content/drive/MyDrive/okra/조리음식목록 및 현황.xlsx', engine='openpyxl') # 엑셀 파일 로드\n",
        "    df = df.iloc[:720,2]\n",
        "    df.dropna(inplace=True) # 널값 제거\n",
        "    names = df.to_list()\n",
        "\n",
        "    device = select_device(device)\n",
        "    model = DetectMultiBackend(weights, device=device, dnn=dnn)\n",
        "    stride,  pt, jit, onnx, engine = model.stride, model.pt, model.jit, model.onnx, model.engine\n",
        "##########################################################################################################\n",
        "    imgsz = check_img_size(imgsz, s=stride)  # check image size\n",
        "\n",
        "    # Half\n",
        "    half &= (pt or jit or engine) and device.type != 'cpu'  # half precision only supported by PyTorch on CUDA\n",
        "    if pt or jit:\n",
        "        model.model.half() if half else model.model.float()\n",
        "\n",
        "    # Dataloader\n",
        "    if webcam:\n",
        "        view_img = check_imshow()\n",
        "        cudnn.benchmark = True  # set True to speed up constant image size inference\n",
        "        dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt)\n",
        "        bs = len(dataset)  # batch_size\n",
        "    else:\n",
        "        dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt)\n",
        "        bs = 1  # batch_size\n",
        "    vid_path, vid_writer = [None] * bs, [None] * bs\n",
        "\n",
        "    # Run inference\n",
        "    model.warmup(imgsz=(1, 3, *imgsz), half=half)  # warmup\n",
        "    dt, seen = [0.0, 0.0, 0.0], 0\n",
        "    for path, im, im0s, vid_cap, s in dataset:\n",
        "        t1 = time_sync()\n",
        "        im = torch.from_numpy(im).to(device)\n",
        "        im = im.half() if half else im.float()  # uint8 to fp16/32\n",
        "        im /= 255  # 0 - 255 to 0.0 - 1.0\n",
        "        if len(im.shape) == 3:\n",
        "            im = im[None]  # expand for batch dim\n",
        "        t2 = time_sync()\n",
        "        dt[0] += t2 - t1\n",
        "\n",
        "        # Inference\n",
        "        visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n",
        "        pred = model(im, augment=augment, visualize=visualize)\n",
        "        t3 = time_sync()\n",
        "        dt[1] += t3 - t2\n",
        "\n",
        "        # NMS\n",
        "        pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
        "\n",
        "      \n",
        "\n",
        "        # print('pred: ',int(pred[0][0][5]))\n",
        "        # pred[0][0][5] = torch.tensor(label_[int(pred[0][0][5])])\n",
        "        # print('change: ', label_[int(pred[0][0][5])])\n",
        "\n",
        "        dt[2] += time_sync() - t3\n",
        "\n",
        "        # Second-stage classifier (optional)\n",
        "        # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)\n",
        "\n",
        "        # Process predictions\n",
        "        for i, det in enumerate(pred):  # per image\n",
        "            seen += 1\n",
        "            if webcam:  # batch_size >= 1\n",
        "                p, im0, frame = path[i], im0s[i].copy(), dataset.count\n",
        "                # print('p:', p, ', im0: ',im0, ',frame: ', frame)\n",
        "                s += f'{i}: '\n",
        "            else:\n",
        "                p, im0, frame = path, im0s.copy(), getattr(dataset, 'frame', 0)\n",
        "                # print('p:', p, ', im0: ',im0, ',frame: ', frame)\n",
        "\n",
        "            p = Path(p)  # to Path\n",
        "            save_path = str(save_dir / p.name)  # im.jpg\n",
        "            txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # im.txt\n",
        "            s += '%gx%g ' % im.shape[2:]  # print string\n",
        "            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
        "            imc = im0.copy() if save_crop else im0  # for save_crop\n",
        "            annotator = Annotator(im0, line_width=line_thickness, example=str(names))\n",
        "            if len(det):\n",
        "                # Rescale boxes from img_size to im0 size\n",
        "                det[:, :4] = scale_coords(im.shape[2:], det[:, :4], im0.shape).round()\n",
        "                # print('det: ', det)\n",
        "\n",
        "                # Print results\n",
        "                for c in det[:, -1].unique():\n",
        "                    n = (det[:, -1] == c).sum()  # detections per class\n",
        "                    s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
        "                    # print('s: ', s)\n",
        "\n",
        "                # Write results\n",
        "                for *xyxy, conf, cls in reversed(det):\n",
        "                    if save_txt:  # Write to file\n",
        "                        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
        "                        line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\n",
        "                        with open(txt_path + '.txt', 'a') as f:\n",
        "                            f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
        "\n",
        "                    if save_img or save_crop or view_img:  # Add bbox to image\n",
        "                        c = int(cls)  # integer class\n",
        "                        label = None if hide_labels else (names[c] if hide_conf else f'{names[c]} {conf:.2f}')\n",
        "                        annotator.box_label(xyxy, label, color=colors(c, True))\n",
        "                        if save_crop:\n",
        "                            save_one_box(xyxy, imc, file=save_dir / 'crops' / names[c] / f'{p.stem}.jpg', BGR=True)\n",
        "\n",
        "            # Print time (inference-only)\n",
        "            LOGGER.info(f'{s}Done. ({t3 - t2:.3f}s)')\n",
        "\n",
        "            # Stream results\n",
        "            im0 = annotator.result()\n",
        "            if view_img:\n",
        "                cv2.imshow(str(p), im0)\n",
        "                cv2.waitKey(1)  # 1 millisecond\n",
        "\n",
        "            # Save results (image with detections)\n",
        "            if save_img:\n",
        "                if dataset.mode == 'image':\n",
        "                    cv2.imwrite(save_path, im0)\n",
        "                else:  # 'video' or 'stream'\n",
        "                    if vid_path[i] != save_path:  # new video\n",
        "                        vid_path[i] = save_path\n",
        "                        if isinstance(vid_writer[i], cv2.VideoWriter):\n",
        "                            vid_writer[i].release()  # release previous video writer\n",
        "                        if vid_cap:  # video\n",
        "                            fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
        "                            w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "                            h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "                        else:  # stream\n",
        "                            fps, w, h = 30, im0.shape[1], im0.shape[0]\n",
        "                            save_path += '.mp4'\n",
        "                        vid_writer[i] = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
        "                    vid_writer[i].write(im0)\n",
        "\n",
        "    # Print results\n",
        "    t = tuple(x / seen * 1E3 for x in dt)  # speeds per image\n",
        "    LOGGER.info(f'Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {(1, 3, *imgsz)}' % t)\n",
        "    if save_txt or save_img:\n",
        "        s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else ''\n",
        "        LOGGER.info(f\"Results saved to {colorstr('bold', save_dir)}{s}\")\n",
        "    if update:\n",
        "        strip_optimizer(weights)  # update model (to fix SourceChangeWarning)\n",
        "\n",
        "\n",
        "def parse_opt():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model path(s)')\n",
        "    parser.add_argument('--source', type=str, default=ROOT / 'data/images', help='file/dir/URL/glob, 0 for webcam')\n",
        "    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640], help='inference size h,w')\n",
        "    parser.add_argument('--conf-thres', type=float, default=0.25, help='confidence threshold')\n",
        "    parser.add_argument('--iou-thres', type=float, default=0.45, help='NMS IoU threshold')\n",
        "    parser.add_argument('--max-det', type=int, default=1000, help='maximum detections per image')\n",
        "    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
        "    parser.add_argument('--view-img', action='store_true', help='show results')\n",
        "    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')\n",
        "    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')\n",
        "    parser.add_argument('--save-crop', action='store_true', help='save cropped prediction boxes')\n",
        "    parser.add_argument('--nosave', action='store_true', help='do not save images/videos')\n",
        "    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --classes 0, or --classes 0 2 3')\n",
        "    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')\n",
        "    parser.add_argument('--augment', action='store_true', help='augmented inference')\n",
        "    parser.add_argument('--visualize', action='store_true', help='visualize features')\n",
        "    parser.add_argument('--update', action='store_true', help='update all models')\n",
        "    parser.add_argument('--project', default=ROOT / 'runs/detect', help='save results to project/name')\n",
        "    parser.add_argument('--name', default='exp', help='save results to project/name')\n",
        "    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n",
        "    parser.add_argument('--line-thickness', default=3, type=int, help='bounding box thickness (pixels)')\n",
        "    parser.add_argument('--hide-labels', default=False, action='store_true', help='hide labels')\n",
        "    parser.add_argument('--hide-conf', default=False, action='store_true', help='hide confidences')\n",
        "    parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference')\n",
        "    parser.add_argument('--dnn', action='store_true', help='use OpenCV DNN for ONNX inference')\n",
        "    opt = parser.parse_args()\n",
        "    opt.imgsz *= 2 if len(opt.imgsz) == 1 else 1  # expand\n",
        "    print_args(FILE.stem, opt)\n",
        "    return opt\n",
        "\n",
        "\n",
        "def main(opt):\n",
        "    check_requirements(exclude=('tensorboard', 'thop'))\n",
        "    run(**vars(opt))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    opt = parse_opt()\n",
        "    main(opt)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw7rHYvsyqPB"
      },
      "source": [
        "# 2. plots.py 교체"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrlgYIYoy9W3"
      },
      "source": [
        "# YOLOv5 🚀 by Ultralytics, GPL-3.0 license\n",
        "\"\"\"\n",
        "Plotting utils\n",
        "\"\"\"\n",
        "\n",
        "# import sys\n",
        "# import io\n",
        "# sys.stdout = io.TextIOWrapper(sys.stdout.detach(), encoding = 'utf-8')\n",
        "# sys.stderr = io.TextIOWrapper(sys.stderr.detach(), encoding = 'utf-8')\n",
        "\n",
        "import math\n",
        "import os\n",
        "from copy import deepcopy\n",
        "from pathlib import Path\n",
        "\n",
        "import cv2\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "import torch\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "from utils.general import (LOGGER, Timeout, check_requirements, clip_coords, increment_path, is_ascii, is_chinese,\n",
        "                           try_except, user_config_dir, xywh2xyxy, xyxy2xywh)\n",
        "from utils.metrics import fitness\n",
        "\n",
        "# Settings\n",
        "CONFIG_DIR = user_config_dir()  # Ultralytics settings dir\n",
        "RANK = int(os.getenv('RANK', -1))\n",
        "matplotlib.rc('font', **{'size': 11})\n",
        "matplotlib.use('Agg')  # for writing to files only\n",
        "\n",
        "\n",
        "class Colors:\n",
        "    # Ultralytics color palette https://ultralytics.com/\n",
        "    def __init__(self):\n",
        "        # hex = matplotlib.colors.TABLEAU_COLORS.values()\n",
        "        hex = ('FF3838', 'FF9D97', 'FF701F', 'FFB21D', 'CFD231', '48F90A', '92CC17', '3DDB86', '1A9334', '00D4BB',\n",
        "               '2C99A8', '00C2FF', '344593', '6473FF', '0018EC', '8438FF', '520085', 'CB38FF', 'FF95C8', 'FF37C7')\n",
        "        self.palette = [self.hex2rgb('#' + c) for c in hex]\n",
        "        self.n = len(self.palette)\n",
        "\n",
        "    def __call__(self, i, bgr=False):\n",
        "        c = self.palette[int(i) % self.n]\n",
        "        return (c[2], c[1], c[0]) if bgr else c\n",
        "\n",
        "    @staticmethod\n",
        "    def hex2rgb(h):  # rgb order (PIL)\n",
        "        return tuple(int(h[1 + i:1 + i + 2], 16) for i in (0, 2, 4))\n",
        "\n",
        "\n",
        "colors = Colors()  # create instance for 'from utils.plots import colors'\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.font_manager as fm\n",
        "fm.get_fontconfig_fonts()\n",
        "font_location = '/content/yolov5/malgun.ttf'\n",
        "font_name = fm.FontProperties(fname=font_location)\n",
        "\n",
        "# matplotlib.rc('font', family=font_name)\n",
        "matplotlib.rcParams['axes.unicode_minus'] = False\n",
        "# 그래프에서 마이너스 폰트 깨지는 문제에 대한 대처\n",
        "\n",
        "######################### font='malgun.ttf'로 교체 #############################################\n",
        "def check_font(font='malgun.ttf', size=10):\n",
        "    # Return a PIL TrueType Font, downloading to CONFIG_DIR if necessary\n",
        "    font = Path(font)\n",
        "    font = font if font.exists() else (CONFIG_DIR / font.name)\n",
        "    try:\n",
        "        return ImageFont.truetype(str(font) if font.exists() else font.name, size)\n",
        "    except Exception as e:  # download if missing\n",
        "        url = \"https://ultralytics.com/assets/\" + font.name\n",
        "        print(f'Downloading {url} to {font}...')\n",
        "        torch.hub.download_url_to_file(url, str(font), progress=False)\n",
        "        try:\n",
        "            return ImageFont.truetype(str(font), size)\n",
        "        except TypeError:\n",
        "            check_requirements('Pillow>=8.4.0')  # known issue https://github.com/ultralytics/yolov5/issues/5374\n",
        "\n",
        "\n",
        "class Annotator:\n",
        "    if RANK in (-1, 0):\n",
        "        check_font()  # download TTF if necessary\n",
        "\n",
        "    # YOLOv5 Annotator for train/val mosaics and jpgs and detect/hub inference annotations\n",
        "######################### font='malgun.ttf'로 교체 #############################################\n",
        "    def __init__(self, im, line_width=None, font_size=None, font='malgun.ttf', pil=False, example='abc'):\n",
        "        assert im.data.contiguous, 'Image not contiguous. Apply np.ascontiguousarray(im) to Annotator() input images.'\n",
        "        self.pil = pil or not is_ascii(example) or is_chinese(example)\n",
        "        if self.pil:  # use PIL\n",
        "            self.im = im if isinstance(im, Image.Image) else Image.fromarray(im)\n",
        "            self.draw = ImageDraw.Draw(self.im)\n",
        "            self.font = check_font(font='Arial.Unicode.ttf' if is_chinese(example) else font,\n",
        "                                   size=font_size or max(round(sum(self.im.size) / 2 * 0.035), 12))\n",
        "        else:  # use cv2\n",
        "            self.im = im\n",
        "        self.lw = line_width or max(round(sum(im.shape) / 2 * 0.003), 2)  # line width\n",
        "\n",
        "    def box_label(self, box, label='', color=(128, 128, 128), txt_color=(255, 255, 255)):\n",
        "        # Add one xyxy box to image with label\n",
        "        if self.pil or not is_ascii(label):\n",
        "            self.draw.rectangle(box, width=self.lw, outline=color)  # box\n",
        "            if label:\n",
        "                w, h = self.font.getsize(label)  # text width, height\n",
        "                outside = box[1] - h >= 0  # label fits outside box\n",
        "                self.draw.rectangle([box[0],\n",
        "                                     box[1] - h if outside else box[1],\n",
        "                                     box[0] + w + 1,\n",
        "                                     box[1] + 1 if outside else box[1] + h + 1], fill=color)\n",
        "                # self.draw.text((box[0], box[1]), label, fill=txt_color, font=self.font, anchor='ls')  # for PIL>8.0\n",
        "                self.draw.text((box[0], box[1] - h if outside else box[1]), label, fill=txt_color, font=self.font)\n",
        "        else:  # cv2\n",
        "            p1, p2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))\n",
        "            cv2.rectangle(self.im, p1, p2, color, thickness=self.lw, lineType=cv2.LINE_AA)\n",
        "            if label:\n",
        "                tf = max(self.lw - 1, 1)  # font thickness\n",
        "                w, h = cv2.getTextSize(label, 0, fontScale=self.lw / 3, thickness=tf)[0]  # text width, height\n",
        "                outside = p1[1] - h - 3 >= 0  # label fits outside box\n",
        "                p2 = p1[0] + w, p1[1] - h - 3 if outside else p1[1] + h + 3\n",
        "                cv2.rectangle(self.im, p1, p2, color, -1, cv2.LINE_AA)  # filled\n",
        "                cv2.putText(self.im, label, (p1[0], p1[1] - 2 if outside else p1[1] + h + 2), 0, self.lw / 3, txt_color,\n",
        "                            thickness=tf, lineType=cv2.LINE_AA)\n",
        "\n",
        "    def rectangle(self, xy, fill=None, outline=None, width=1):\n",
        "        # Add rectangle to image (PIL-only)\n",
        "        self.draw.rectangle(xy, fill, outline, width)\n",
        "\n",
        "    def text(self, xy, text, txt_color=(255, 255, 255)):\n",
        "        # Add text to image (PIL-only)\n",
        "        w, h = self.font.getsize(text)  # text width, height\n",
        "        self.draw.text((xy[0], xy[1] - h + 1), text, fill=txt_color, font=self.font)\n",
        "\n",
        "    def result(self):\n",
        "        # Return annotated image as array\n",
        "        return np.asarray(self.im)\n",
        "\n",
        "\n",
        "def feature_visualization(x, module_type, stage, n=32, save_dir=Path('runs/detect/exp')):\n",
        "    \"\"\"\n",
        "    x:              Features to be visualized\n",
        "    module_type:    Module type\n",
        "    stage:          Module stage within model\n",
        "    n:              Maximum number of feature maps to plot\n",
        "    save_dir:       Directory to save results\n",
        "    \"\"\"\n",
        "    if 'Detect' not in module_type:\n",
        "        batch, channels, height, width = x.shape  # batch, channels, height, width\n",
        "        if height > 1 and width > 1:\n",
        "            f = save_dir / f\"stage{stage}_{module_type.split('.')[-1]}_features.png\"  # filename\n",
        "\n",
        "            blocks = torch.chunk(x[0].cpu(), channels, dim=0)  # select batch index 0, block by channels\n",
        "            n = min(n, channels)  # number of plots\n",
        "            fig, ax = plt.subplots(math.ceil(n / 8), 8, tight_layout=True)  # 8 rows x n/8 cols\n",
        "            ax = ax.ravel()\n",
        "            plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
        "            for i in range(n):\n",
        "                ax[i].imshow(blocks[i].squeeze())  # cmap='gray'\n",
        "                ax[i].axis('off')\n",
        "\n",
        "            print(f'Saving {f}... ({n}/{channels})')\n",
        "            plt.savefig(f, dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "            np.save(str(f.with_suffix('.npy')), x[0].cpu().numpy())  # npy save\n",
        "\n",
        "\n",
        "def hist2d(x, y, n=100):\n",
        "    # 2d histogram used in labels.png and evolve.png\n",
        "    xedges, yedges = np.linspace(x.min(), x.max(), n), np.linspace(y.min(), y.max(), n)\n",
        "    hist, xedges, yedges = np.histogram2d(x, y, (xedges, yedges))\n",
        "    xidx = np.clip(np.digitize(x, xedges) - 1, 0, hist.shape[0] - 1)\n",
        "    yidx = np.clip(np.digitize(y, yedges) - 1, 0, hist.shape[1] - 1)\n",
        "    return np.log(hist[xidx, yidx])\n",
        "\n",
        "\n",
        "def butter_lowpass_filtfilt(data, cutoff=1500, fs=50000, order=5):\n",
        "    from scipy.signal import butter, filtfilt\n",
        "\n",
        "    # https://stackoverflow.com/questions/28536191/how-to-filter-smooth-with-scipy-numpy\n",
        "    def butter_lowpass(cutoff, fs, order):\n",
        "        nyq = 0.5 * fs\n",
        "        normal_cutoff = cutoff / nyq\n",
        "        return butter(order, normal_cutoff, btype='low', analog=False)\n",
        "\n",
        "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
        "    return filtfilt(b, a, data)  # forward-backward filter\n",
        "\n",
        "\n",
        "def output_to_target(output):\n",
        "    # Convert model output to target format [batch_id, class_id, x, y, w, h, conf]\n",
        "    targets = []\n",
        "    for i, o in enumerate(output):\n",
        "        for *box, conf, cls in o.cpu().numpy():\n",
        "            targets.append([i, cls, *list(*xyxy2xywh(np.array(box)[None])), conf])\n",
        "    return np.array(targets)\n",
        "\n",
        "\n",
        "def plot_images(images, targets, paths=None, fname='images.jpg', names=None, max_size=1920, max_subplots=16):\n",
        "    # Plot image grid with labels\n",
        "    if isinstance(images, torch.Tensor):\n",
        "        images = images.cpu().float().numpy()\n",
        "    if isinstance(targets, torch.Tensor):\n",
        "        targets = targets.cpu().numpy()\n",
        "    if np.max(images[0]) <= 1:\n",
        "        images *= 255  # de-normalise (optional)\n",
        "    bs, _, h, w = images.shape  # batch size, _, height, width\n",
        "    bs = min(bs, max_subplots)  # limit plot images\n",
        "    ns = np.ceil(bs ** 0.5)  # number of subplots (square)\n",
        "\n",
        "    # Build Image\n",
        "    mosaic = np.full((int(ns * h), int(ns * w), 3), 255, dtype=np.uint8)  # init\n",
        "    for i, im in enumerate(images):\n",
        "        if i == max_subplots:  # if last batch has fewer images than we expect\n",
        "            break\n",
        "        x, y = int(w * (i // ns)), int(h * (i % ns))  # block origin\n",
        "        im = im.transpose(1, 2, 0)\n",
        "        mosaic[y:y + h, x:x + w, :] = im\n",
        "\n",
        "    # Resize (optional)\n",
        "    scale = max_size / ns / max(h, w)\n",
        "    if scale < 1:\n",
        "        h = math.ceil(scale * h)\n",
        "        w = math.ceil(scale * w)\n",
        "        mosaic = cv2.resize(mosaic, tuple(int(x * ns) for x in (w, h)))\n",
        "\n",
        "    # Annotate\n",
        "    fs = int((h + w) * ns * 0.01)  # font size\n",
        "    annotator = Annotator(mosaic, line_width=round(fs / 10), font_size=fs, pil=True)\n",
        "    for i in range(i + 1):\n",
        "        x, y = int(w * (i // ns)), int(h * (i % ns))  # block origin\n",
        "        annotator.rectangle([x, y, x + w, y + h], None, (255, 255, 255), width=2)  # borders\n",
        "        if paths:\n",
        "            annotator.text((x + 5, y + 5 + h), text=Path(paths[i]).name[:40], txt_color=(220, 220, 220))  # filenames\n",
        "        if len(targets) > 0:\n",
        "            ti = targets[targets[:, 0] == i]  # image targets\n",
        "            boxes = xywh2xyxy(ti[:, 2:6]).T\n",
        "            classes = ti[:, 1].astype('int')\n",
        "            labels = ti.shape[1] == 6  # labels if no conf column\n",
        "            conf = None if labels else ti[:, 6]  # check for confidence presence (label vs pred)\n",
        "\n",
        "            if boxes.shape[1]:\n",
        "                if boxes.max() <= 1.01:  # if normalized with tolerance 0.01\n",
        "                    boxes[[0, 2]] *= w  # scale to pixels\n",
        "                    boxes[[1, 3]] *= h\n",
        "                elif scale < 1:  # absolute coords need scale if image scales\n",
        "                    boxes *= scale\n",
        "            boxes[[0, 2]] += x\n",
        "            boxes[[1, 3]] += y\n",
        "            for j, box in enumerate(boxes.T.tolist()):\n",
        "                cls = classes[j]\n",
        "                color = colors(cls)\n",
        "                cls = names[cls] if names else cls\n",
        "                if labels or conf[j] > 0.25:  # 0.25 conf thresh\n",
        "                    label = f'{cls}' if labels else f'{cls} {conf[j]:.1f}'\n",
        "                    annotator.box_label(box, label, color=color)\n",
        "    annotator.im.save(fname)  # save\n",
        "\n",
        "\n",
        "def plot_lr_scheduler(optimizer, scheduler, epochs=300, save_dir=''):\n",
        "    # Plot LR simulating training for full epochs\n",
        "    optimizer, scheduler = deepcopy(optimizer), deepcopy(scheduler)  # do not modify originals\n",
        "    y = []\n",
        "    for _ in range(epochs):\n",
        "        scheduler.step()\n",
        "        y.append(optimizer.param_groups[0]['lr'])\n",
        "    plt.plot(y, '.-', label='LR')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('LR')\n",
        "    plt.grid()\n",
        "    plt.xlim(0, epochs)\n",
        "    plt.ylim(0)\n",
        "    plt.savefig(Path(save_dir) / 'LR.png', dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_val_txt():  # from utils.plots import *; plot_val()\n",
        "    # Plot val.txt histograms\n",
        "    x = np.loadtxt('val.txt', dtype=np.float32)\n",
        "    box = xyxy2xywh(x[:, :4])\n",
        "    cx, cy = box[:, 0], box[:, 1]\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(6, 6), tight_layout=True)\n",
        "    ax.hist2d(cx, cy, bins=600, cmax=10, cmin=0)\n",
        "    ax.set_aspect('equal')\n",
        "    plt.savefig('hist2d.png', dpi=300)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(12, 6), tight_layout=True)\n",
        "    ax[0].hist(cx, bins=600)\n",
        "    ax[1].hist(cy, bins=600)\n",
        "    plt.savefig('hist1d.png', dpi=200)\n",
        "\n",
        "\n",
        "def plot_targets_txt():  # from utils.plots import *; plot_targets_txt()\n",
        "    # Plot targets.txt histograms\n",
        "    x = np.loadtxt('targets.txt', dtype=np.float32).T\n",
        "    s = ['x targets', 'y targets', 'width targets', 'height targets']\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(8, 8), tight_layout=True)\n",
        "    ax = ax.ravel()\n",
        "    for i in range(4):\n",
        "        ax[i].hist(x[i], bins=100, label=f'{x[i].mean():.3g} +/- {x[i].std():.3g}')\n",
        "        ax[i].legend()\n",
        "        ax[i].set_title(s[i])\n",
        "    plt.savefig('targets.jpg', dpi=200)\n",
        "\n",
        "\n",
        "def plot_val_study(file='', dir='', x=None):  # from utils.plots import *; plot_val_study()\n",
        "    # Plot file=study.txt generated by val.py (or plot all study*.txt in dir)\n",
        "    save_dir = Path(file).parent if file else Path(dir)\n",
        "    plot2 = False  # plot additional results\n",
        "    if plot2:\n",
        "        ax = plt.subplots(2, 4, figsize=(10, 6), tight_layout=True)[1].ravel()\n",
        "\n",
        "    fig2, ax2 = plt.subplots(1, 1, figsize=(8, 4), tight_layout=True)\n",
        "    # for f in [save_dir / f'study_coco_{x}.txt' for x in ['yolov5n6', 'yolov5s6', 'yolov5m6', 'yolov5l6', 'yolov5x6']]:\n",
        "    for f in sorted(save_dir.glob('study*.txt')):\n",
        "        y = np.loadtxt(f, dtype=np.float32, usecols=[0, 1, 2, 3, 7, 8, 9], ndmin=2).T\n",
        "        x = np.arange(y.shape[1]) if x is None else np.array(x)\n",
        "        if plot2:\n",
        "            s = ['P', 'R', 'mAP@.5', 'mAP@.5:.95', 't_preprocess (ms/img)', 't_inference (ms/img)', 't_NMS (ms/img)']\n",
        "            for i in range(7):\n",
        "                ax[i].plot(x, y[i], '.-', linewidth=2, markersize=8)\n",
        "                ax[i].set_title(s[i])\n",
        "\n",
        "        j = y[3].argmax() + 1\n",
        "        ax2.plot(y[5, 1:j], y[3, 1:j] * 1E2, '.-', linewidth=2, markersize=8,\n",
        "                 label=f.stem.replace('study_coco_', '').replace('yolo', 'YOLO'))\n",
        "\n",
        "    ax2.plot(1E3 / np.array([209, 140, 97, 58, 35, 18]), [34.6, 40.5, 43.0, 47.5, 49.7, 51.5],\n",
        "             'k.-', linewidth=2, markersize=8, alpha=.25, label='EfficientDet')\n",
        "\n",
        "    ax2.grid(alpha=0.2)\n",
        "    ax2.set_yticks(np.arange(20, 60, 5))\n",
        "    ax2.set_xlim(0, 57)\n",
        "    ax2.set_ylim(25, 55)\n",
        "    ax2.set_xlabel('GPU Speed (ms/img)')\n",
        "    ax2.set_ylabel('COCO AP val')\n",
        "    ax2.legend(loc='lower right')\n",
        "    f = save_dir / 'study.png'\n",
        "    print(f'Saving {f}...')\n",
        "    plt.savefig(f, dpi=300)\n",
        "\n",
        "\n",
        "@try_except  # known issue https://github.com/ultralytics/yolov5/issues/5395\n",
        "@Timeout(30)  # known issue https://github.com/ultralytics/yolov5/issues/5611\n",
        "def plot_labels(labels, names=(), save_dir=Path('')):\n",
        "    # plot dataset labels\n",
        "    LOGGER.info(f\"Plotting labels to {save_dir / 'labels.jpg'}... \")\n",
        "    c, b = labels[:, 0], labels[:, 1:].transpose()  # classes, boxes\n",
        "    nc = int(c.max() + 1)  # number of classes\n",
        "    x = pd.DataFrame(b.transpose(), columns=['x', 'y', 'width', 'height'])\n",
        "\n",
        "    # seaborn correlogram\n",
        "    sn.pairplot(x, corner=True, diag_kind='auto', kind='hist', diag_kws=dict(bins=50), plot_kws=dict(pmax=0.9))\n",
        "    plt.savefig(save_dir / 'labels_correlogram.jpg', dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "    # matplotlib labels\n",
        "    matplotlib.use('svg')  # faster\n",
        "    ax = plt.subplots(2, 2, figsize=(8, 8), tight_layout=True)[1].ravel()\n",
        "    y = ax[0].hist(c, bins=np.linspace(0, nc, nc + 1) - 0.5, rwidth=0.8)\n",
        "    # [y[2].patches[i].set_color([x / 255 for x in colors(i)]) for i in range(nc)]  # update colors bug #3195\n",
        "    ax[0].set_ylabel('instances')\n",
        "    if 0 < len(names) < 30:\n",
        "        ax[0].set_xticks(range(len(names)))\n",
        "        ax[0].set_xticklabels(names, rotation=90, fontsize=10)\n",
        "    else:\n",
        "        ax[0].set_xlabel('classes')\n",
        "    sn.histplot(x, x='x', y='y', ax=ax[2], bins=50, pmax=0.9)\n",
        "    sn.histplot(x, x='width', y='height', ax=ax[3], bins=50, pmax=0.9)\n",
        "\n",
        "    # rectangles\n",
        "    labels[:, 1:3] = 0.5  # center\n",
        "    labels[:, 1:] = xywh2xyxy(labels[:, 1:]) * 2000\n",
        "    img = Image.fromarray(np.ones((2000, 2000, 3), dtype=np.uint8) * 255)\n",
        "    for cls, *box in labels[:1000]:\n",
        "        ImageDraw.Draw(img).rectangle(box, width=1, outline=colors(cls))  # plot\n",
        "    ax[1].imshow(img)\n",
        "    ax[1].axis('off')\n",
        "\n",
        "    for a in [0, 1, 2, 3]:\n",
        "        for s in ['top', 'right', 'left', 'bottom']:\n",
        "            ax[a].spines[s].set_visible(False)\n",
        "\n",
        "    plt.savefig(save_dir / 'labels.jpg', dpi=200)\n",
        "    matplotlib.use('Agg')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_evolve(evolve_csv='path/to/evolve.csv'):  # from utils.plots import *; plot_evolve()\n",
        "    # Plot evolve.csv hyp evolution results\n",
        "    evolve_csv = Path(evolve_csv)\n",
        "    data = pd.read_csv(evolve_csv)\n",
        "    keys = [x.strip() for x in data.columns]\n",
        "    x = data.values\n",
        "    f = fitness(x)\n",
        "    j = np.argmax(f)  # max fitness index\n",
        "    plt.figure(figsize=(10, 12), tight_layout=True)\n",
        "    matplotlib.rc('font', **{'size': 8})\n",
        "    for i, k in enumerate(keys[7:]):\n",
        "        v = x[:, 7 + i]\n",
        "        mu = v[j]  # best single result\n",
        "        plt.subplot(6, 5, i + 1)\n",
        "        plt.scatter(v, f, c=hist2d(v, f, 20), cmap='viridis', alpha=.8, edgecolors='none')\n",
        "        plt.plot(mu, f.max(), 'k+', markersize=15)\n",
        "        plt.title(f'{k} = {mu:.3g}', fontdict={'size': 9})  # limit to 40 characters\n",
        "        if i % 5 != 0:\n",
        "            plt.yticks([])\n",
        "        print(f'{k:>15}: {mu:.3g}')\n",
        "    f = evolve_csv.with_suffix('.png')  # filename\n",
        "    plt.savefig(f, dpi=200)\n",
        "    plt.close()\n",
        "    print(f'Saved {f}')\n",
        "\n",
        "\n",
        "def plot_results(file='path/to/results.csv', dir=''):\n",
        "    # Plot training results.csv. Usage: from utils.plots import *; plot_results('path/to/results.csv')\n",
        "    save_dir = Path(file).parent if file else Path(dir)\n",
        "    fig, ax = plt.subplots(2, 5, figsize=(12, 6), tight_layout=True)\n",
        "    ax = ax.ravel()\n",
        "    files = list(save_dir.glob('results*.csv'))\n",
        "    assert len(files), f'No results.csv files found in {save_dir.resolve()}, nothing to plot.'\n",
        "    for fi, f in enumerate(files):\n",
        "        try:\n",
        "            data = pd.read_csv(f)\n",
        "            s = [x.strip() for x in data.columns]\n",
        "            x = data.values[:, 0]\n",
        "            for i, j in enumerate([1, 2, 3, 4, 5, 8, 9, 10, 6, 7]):\n",
        "                y = data.values[:, j]\n",
        "                # y[y == 0] = np.nan  # don't show zero values\n",
        "                ax[i].plot(x, y, marker='.', label=f.stem, linewidth=2, markersize=8)\n",
        "                ax[i].set_title(s[j], fontsize=12)\n",
        "                # if j in [8, 9, 10]:  # share train and val loss y axes\n",
        "                #     ax[i].get_shared_y_axes().join(ax[i], ax[i - 5])\n",
        "        except Exception as e:\n",
        "            print(f'Warning: Plotting error for {f}: {e}')\n",
        "    ax[1].legend()\n",
        "    fig.savefig(save_dir / 'results.png', dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def profile_idetection(start=0, stop=0, labels=(), save_dir=''):\n",
        "    # Plot iDetection '*.txt' per-image logs. from utils.plots import *; profile_idetection()\n",
        "    ax = plt.subplots(2, 4, figsize=(12, 6), tight_layout=True)[1].ravel()\n",
        "    s = ['Images', 'Free Storage (GB)', 'RAM Usage (GB)', 'Battery', 'dt_raw (ms)', 'dt_smooth (ms)', 'real-world FPS']\n",
        "    files = list(Path(save_dir).glob('frames*.txt'))\n",
        "    for fi, f in enumerate(files):\n",
        "        try:\n",
        "            results = np.loadtxt(f, ndmin=2).T[:, 90:-30]  # clip first and last rows\n",
        "            n = results.shape[1]  # number of rows\n",
        "            x = np.arange(start, min(stop, n) if stop else n)\n",
        "            results = results[:, x]\n",
        "            t = (results[0] - results[0].min())  # set t0=0s\n",
        "            results[0] = x\n",
        "            for i, a in enumerate(ax):\n",
        "                if i < len(results):\n",
        "                    label = labels[fi] if len(labels) else f.stem.replace('frames_', '')\n",
        "                    a.plot(t, results[i], marker='.', label=label, linewidth=1, markersize=5)\n",
        "                    a.set_title(s[i])\n",
        "                    a.set_xlabel('time (s)')\n",
        "                    # if fi == len(files) - 1:\n",
        "                    #     a.set_ylim(bottom=0)\n",
        "                    for side in ['top', 'right']:\n",
        "                        a.spines[side].set_visible(False)\n",
        "                else:\n",
        "                    a.remove()\n",
        "        except Exception as e:\n",
        "            print(f'Warning: Plotting error for {f}; {e}')\n",
        "    ax[1].legend()\n",
        "    plt.savefig(Path(save_dir) / 'idetection_profile.png', dpi=200)\n",
        "\n",
        "\n",
        "def save_one_box(xyxy, im, file='image.jpg', gain=1.02, pad=10, square=False, BGR=False, save=True):\n",
        "    # Save image crop as {file} with crop size multiple {gain} and {pad} pixels. Save and/or return crop\n",
        "    xyxy = torch.tensor(xyxy).view(-1, 4)\n",
        "    b = xyxy2xywh(xyxy)  # boxes\n",
        "    if square:\n",
        "        b[:, 2:] = b[:, 2:].max(1)[0].unsqueeze(1)  # attempt rectangle to square\n",
        "    b[:, 2:] = b[:, 2:] * gain + pad  # box wh * gain + pad\n",
        "    xyxy = xywh2xyxy(b).long()\n",
        "    clip_coords(xyxy, im.shape)\n",
        "    crop = im[int(xyxy[0, 1]):int(xyxy[0, 3]), int(xyxy[0, 0]):int(xyxy[0, 2]), ::(1 if BGR else -1)]\n",
        "    if save:\n",
        "        file.parent.mkdir(parents=True, exist_ok=True)  # make directory\n",
        "        cv2.imwrite(str(increment_path(file).with_suffix('.jpg')), crop)\n",
        "    return crop\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "Brfhzy4qiuGV",
        "outputId": "64c7deb0-e09e-45bc-9c81-b5ae2939de8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzxaLKiybwkY",
        "outputId": "295486e7-8d80-4191-ab07-0de27fef23ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python detect.py --weights /content/drive/MyDrive/okra/food689_best_ep1000.pt --img 640 --conf 0.25 --source /content/drive/MyDrive/okra/test/image5.jpg --device 0 --project /content/drive/MyDrive/okra/result"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/drive/MyDrive/okra/food689_best_ep1000.pt'], source=/content/drive/MyDrive/okra/test/image5.jpg, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=0, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=/content/drive/MyDrive/okra/result, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
            "YOLOv5 🚀 v6.0-127-g554f782 torch 1.10.0+cu111 CUDA:0 (Tesla K80, 11441MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 484 layers, 93228765 parameters, 0 gradients\n",
            "image 1/1 /content/drive/MyDrive/okra/test/image5.jpg: 480x640 1 미역나물, Done. (0.238s)\n",
            "Speed: 0.7ms pre-process, 237.9ms inference, 2.3ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/okra/result/exp14\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4nvw71xbwfM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_gAUKsmbwck"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}